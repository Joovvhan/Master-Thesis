{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning Clean Up.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Joovvhan/Master-Thesis/blob/master/log/A3F1P3-A3F5P3_VGG16_Test_2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "c3821KWLNey6",
        "colab_type": "code",
        "outputId": "73da543d-ec4c-41ac-fe3f-3a795fa09e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wf\n",
        "import time\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "# Import Keras modules\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nxSASpgF7unA",
        "colab_type": "code",
        "outputId": "1343db1f-7452-4004-d788-48664c1f074a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.listdir('gdrive/My Drive/Colab')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data_', 'Model', 'Data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "rw9NU8Xjwcz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set data directories\n",
        "\n",
        "dataPath = 'gdrive/My Drive/Colab/Data'\n",
        "\n",
        "# Changed variable names to normal and fault\n",
        "# Changed variable names from folder to path\n",
        "# Need to consider multiple folders\n",
        "# Need to add files to be tested\n",
        "\n",
        "# folderNormal = ['A1F1P1', 'A2F1P1', 'A1F2P1', 'A1F1P2', 'A2F2P1', 'A1F2P2', 'A2F1P2', 'A2F2P2']\n",
        "folderNormal = ['A3F1P3']\n",
        "\n",
        "folderFault = ['A3F5P3']\n",
        "\n",
        "# for x in range(1, 6):\n",
        "#     for y in range(1, 6):\n",
        "#         for z in range(1, 6):\n",
        "#             if (x == 5 or y == 5 or z == 5):\n",
        "#                 folderFault.append('A{}F{}P{}'.format(x, y, z))\n",
        "\n",
        "\n",
        "pathNormal = list()\n",
        "pathFault = list()\n",
        "\n",
        "for i in range(len(folderNormal)):\n",
        "    pathNormal.append(dataPath + '/' + folderNormal[i])\n",
        "    \n",
        "for i in range(len(folderFault)):\n",
        "    pathFault.append(dataPath + '/' + folderFault[i])\n",
        "\n",
        "# normIdx = list()\n",
        "# faultIdx = list()\n",
        "# filesNormal = list()\n",
        "# filesFault = list()\n",
        "\n",
        "# for i in range(len(pathNormal)):\n",
        "#     filesNormalTemp = os.listdir(pathNormal[i])\n",
        "#     filesNormalTemp = [file for file in filesNormalTemp if file.endswith('.wav')]\n",
        "    \n",
        "#     randIdx = np.random.choice(range(1000), round(1000/len(pathNormal)), replace=False)\n",
        "#     randIdx.sort()\n",
        "#     normIdx.append(randIdx)\n",
        "    \n",
        "#     filesNormalSelected = [pathNormal[i] + '/' + filesNormalTemp[j] for j in randIdx]\n",
        "    \n",
        "#     filesNormal = filesNormal + filesNormalSelected\n",
        "    \n",
        "    \n",
        "# for i in range(len(pathFault)):\n",
        "#     filesFaultTemp = os.listdir(pathFault[i])\n",
        "#     filesFaultTemp = [file for file in filesFaultTemp if file.endswith('.wav')]\n",
        "    \n",
        "#     randIdx = np.random.choice(range(1000), round(1000/len(pathFault)), replace=False)\n",
        "#     randIdx.sort()\n",
        "#     faultIdx.append(randIdx)\n",
        "    \n",
        "#     filesFaultSelected = [pathFault[i] + '/' + filesFaultTemp[j] for j in randIdx]\n",
        "    \n",
        "#     filesFault = filesFault + filesFaultSelected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgGP7PZO-rTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specgram settings\n",
        "\n",
        "nsc = 1470\n",
        "nov = nsc/2\n",
        "nff = nsc \n",
        "imgSize = 224\n",
        "\n",
        "pretrainedModel = 'VGG16'\n",
        "# pretrainedModel = 'Xception'\n",
        "# pretrainedModel = 'ResNet50'\n",
        "\n",
        "lastActivation = 'softmax'\n",
        "sizeBatch = 16\n",
        "numEpochs = 8\n",
        "verb = 1\n",
        "\n",
        "\n",
        "# Learning parameters\n",
        "\n",
        "trainingRatio = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AskF2qFT2xr",
        "colab_type": "code",
        "outputId": "5b1def53-4d5a-4270-a525-3cb88a72983d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Execution confirmed with new variable names\n",
        "\n",
        "# Check whether npy file exists \n",
        "\n",
        "# npyNormalPath = glob.glob(pathNormal + '/' + '*.npy')\n",
        "\n",
        "# if (len(npyNormalPath) == 1):\n",
        "#     imgsNormal = np.load(npyNormalPath[0])\n",
        "\n",
        "# else:\n",
        "#     imgsNormal = np.zeros([len(filesNormal), imgSize, imgSize])\n",
        "    \n",
        "#     for i in trange(len(filesNormal)):\n",
        "#         fs, dataInt16 = wf.read(pathNormal + '/' + filesNormal[i])\n",
        "#         dataFloat = dataInt16 / (2 ** 15)\n",
        "#         Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "#                                            window=np.hamming(nsc), cmap='viridis')\n",
        "#         plt.close()\n",
        "#         imgsNormal[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "#     np.save(pathNormal + '/' + folderNormal + '.npy', imgsNormal)\n",
        "        \n",
        "# print('Normal Image Shape: {}'.format(imgsNormal.shape))\n",
        "\n",
        "# if len(folderNormal) == 1:\n",
        "#     npyNormalPath = glob.glob(pathNormal[0] + '/' + '*.npy')\n",
        "# else:\n",
        "#     npyNormalPath = list()\n",
        "    \n",
        "# if (len(npyNormalPath) == 1):\n",
        "#     imgsNormal = np.load(npyNormalPath[0])\n",
        "\n",
        "# else:\n",
        "#     imgsNormal = np.zeros([len(filesNormal), imgSize, imgSize])\n",
        "    \n",
        "#     for i in trange(len(filesNormal)):\n",
        "#         fs, dataInt16 = wf.read(filesNormal[i])\n",
        "#         dataFloat = dataInt16 / (2 ** 15)\n",
        "#         Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "#                                            window=np.hamming(nsc), cmap='viridis')\n",
        "#         plt.close()\n",
        "#         imgsNormal[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "# #     np.save(pathNormal + '/' + folderNormal + '.npy', imgsNormal)\n",
        "        \n",
        "# print('Normal Image Shape: {}'.format(imgsNormal.shape))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "startNum = 0\n",
        "\n",
        "for i in range(startNum, len(pathNormal)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathNormal[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "    \n",
        "    label = data[:, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelList = label\n",
        "    else:\n",
        "        labelList = np.vstack([labelList, label])\n",
        "\n",
        "    print('Normal Image Shape From {}: {}'.format(pathNormal[i], data.shape))    \n",
        "\n",
        "    \n",
        "imgsNormal = imgs\n",
        "normalLabel = label\n",
        "    \n",
        "print('Normal Image Shape: {}'.format(imgsNormal.shape))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A3F1P3: (1000, 5)\n",
            "Normal Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_MZtpk3Br_i",
        "colab_type": "code",
        "outputId": "f52d4192-5feb-4e13-f3f7-50152c2556ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Execution confined with new varible names\n",
        "\n",
        "# Check whether npy file exists\n",
        "\n",
        "# npyFaultPath = glob.glob(pathFault + '/' + '*.npy')\n",
        "\n",
        "# if (len(npyFaultPath) == 1):\n",
        "#     imgsFault = np.load(npyFaultPath[0])\n",
        "\n",
        "# else:\n",
        "#     imgsFault = np.zeros([len(filesFault), imgSize, imgSize])\n",
        "\n",
        "#     for i in trange(len(filesFault)):\n",
        "#         fs, dataInt16 = wf.read(pathFault + '/' + filesFault[i])\n",
        "#         dataFloat = dataInt16 / (2 ** 15)\n",
        "#         Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "#                                            window=np.hamming(nsc), cmap='viridis')\n",
        "#         plt.close()\n",
        "#         imgsFault[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "#     np.save(pathFault + '/' + folderFault + '.npy', imgsFault)\n",
        "        \n",
        "# print('Fault Image Shape: {}'.format(imgsFault.shape))\n",
        "\n",
        "# if len(folderFault) == 1:\n",
        "#     npyFaultPath = glob.glob(pathFault[0] + '/' + '*.npy')\n",
        "# else:\n",
        "#     npyFaultPath = list()\n",
        "\n",
        "# if (len(npyFaultPath) == 1):\n",
        "#     imgsFault = np.load(npyFaultPath[0])\n",
        "\n",
        "# else:\n",
        "#     imgsFault = np.zeros([len(filesFault), imgSize, imgSize])\n",
        "\n",
        "#     for i in trange(len(filesFault)):\n",
        "#         fs, dataInt16 = wf.read(filesFault[i])\n",
        "#         dataFloat = dataInt16 / (2 ** 15)\n",
        "#         Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "#                                            window=np.hamming(nsc), cmap='viridis')\n",
        "#         plt.close()\n",
        "#         imgsFault[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "# #     np.save(pathFault + '/' + folderFault + '.npy', imgsFault)\n",
        "        \n",
        "# print('Fault Image Shape: {}'.format(imgsFault.shape))\n",
        "\n",
        "startNum = 0\n",
        "\n",
        "for i in range(startNum, len(pathFault)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathFault[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "    \n",
        "    label = data[:, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelList = label\n",
        "    else:\n",
        "        labelList = np.vstack([labelList, label])\n",
        "\n",
        "    print('Fault Image Shape From {}: {}'.format(pathFault[i], data.shape))    \n",
        "\n",
        "    \n",
        "imgsFault = imgs\n",
        "faultLabel = label\n",
        "    \n",
        "print('Fault Image Shape: {}'.format(imgsFault.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A3F5P3: (1000, 5)\n",
            "Fault Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e91QsAurSkIP",
        "colab_type": "code",
        "outputId": "e5ae5511-4ace-4b83-be97-914a6ecea7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "# Change name from imgsF1 or imgsF5 to imgsNormal and imgsFault\n",
        "\n",
        "dataNumNormal = len(imgsNormal)\n",
        "dataNumFault = len(imgsFault)\n",
        "dataNumNormalTrain = int(dataNumNormal * trainingRatio)\n",
        "dataNumFaultTrain = int(dataNumFault * trainingRatio)\n",
        "dataNumNormalTest = dataNumNormal - dataNumNormalTrain\n",
        "dataNumFaultTest = dataNumFault - dataNumFaultTrain\n",
        "\n",
        "print('Normal Train:Test = {:d}:{:d}'.format(dataNumNormalTrain, dataNumNormalTest))\n",
        "print('Fault  Train:Test = {:d}:{:d}\\n'.format(dataNumFaultTrain, dataNumFaultTest))\n",
        "\n",
        "trainIdxNormal = np.random.choice(dataNumNormal - 1, dataNumNormalTrain, replace=False)\n",
        "testIdxNormal = list(set(range(0, dataNumNormal)) - set(trainIdxNormal))\n",
        "\n",
        "trainImgsNormal = imgsNormal[trainIdxNormal, :, :]\n",
        "testImgsNormal = imgsNormal[testIdxNormal, :, :]\n",
        "\n",
        "print('Normal Training Image Shape {}'.format(trainImgsNormal.shape))\n",
        "print('Normal Test Image Shape {}\\n'.format(testImgsNormal.shape))\n",
        "\n",
        "trainIdxFault  = np.random.choice(dataNumFault - 1, dataNumFaultTrain, replace=False)\n",
        "testIdxFault = list(set(range(0, dataNumFault)) - set(trainIdxFault))\n",
        "\n",
        "trainImgsFault = imgsFault[trainIdxFault, :, :]\n",
        "testImgsFault = imgsFault[testIdxFault, :, :]\n",
        "\n",
        "print('Fault Training Image Shape {}'.format(trainImgsFault.shape))\n",
        "print('Fault Test Image Shape {}\\n'.format(testImgsFault.shape))\n",
        "\n",
        "trainImgs = np.vstack([trainImgsNormal, trainImgsFault])\n",
        "testImgs = np.vstack([testImgsNormal, testImgsFault])\n",
        "\n",
        "print('Training Image Shape {}'.format(trainImgs.shape))\n",
        "print('Test Image Shape {}'.format(testImgs.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Train:Test = 800:200\n",
            "Fault  Train:Test = 800:200\n",
            "\n",
            "Normal Training Image Shape (800, 224, 224)\n",
            "Normal Test Image Shape (200, 224, 224)\n",
            "\n",
            "Fault Training Image Shape (800, 224, 224)\n",
            "Fault Test Image Shape (200, 224, 224)\n",
            "\n",
            "Training Image Shape (1600, 224, 224)\n",
            "Test Image Shape (400, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "feQ_AHAcUgHD",
        "colab_type": "code",
        "outputId": "8a9fe162-1341-478f-e000-609b598a138b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainMean = np.mean(trainImgs)\n",
        "trainStd = np.std(trainImgs)\n",
        "\n",
        "print('Mean of Training Image: {}'.format(trainMean))\n",
        "print('Standard Deviation of Training Image: {}'.format(trainStd))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Training Image: -78.09799818542011\n",
            "Standard Deviation of Training Image: 9.39065389499644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wUi9IuXNUpir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Should Change Norm to Normalized\n",
        "\n",
        "trainImgsNorm = (trainImgs - trainMean) / trainStd\n",
        "testImgsNorm = (testImgs - trainMean) / trainStd\n",
        "\n",
        "trainImgsNorm = trainImgsNorm.reshape(list(trainImgsNorm.shape) + [1])\n",
        "testImgsNorm = testImgsNorm.reshape(list(testImgsNorm.shape) + [1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aklxiPz1Ve4n",
        "colab_type": "code",
        "outputId": "170554fc-f473-4180-f9b6-33eee65dee2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = np.stack([trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0]], axis = -1)\n",
        "X_test = np.stack([testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0]], axis = -1)\n",
        "\n",
        "print('X_train Shape: {}'.format(X_train.shape))\n",
        "print('X_test  Shape: {}'.format(X_test.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train Shape: (1600, 224, 224, 3)\n",
            "X_test  Shape: (400, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRgIPzT7K_YP",
        "colab_type": "code",
        "outputId": "b51368d2-8257-4265-9fba-fb8c31df3201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainLabelNormal = np.stack((np.ones(dataNumNormalTrain), np.zeros(dataNumNormalTrain)), axis = -1)\n",
        "testLabelNormal = np.stack((np.ones(dataNumNormalTest), np.zeros(dataNumNormalTest)), axis = -1)\n",
        "\n",
        "trainLabelFault = np.stack((np.zeros(dataNumFaultTrain), np.ones(dataNumFaultTrain)), axis = -1)\n",
        "testLabelFault = np.stack((np.zeros(dataNumFaultTest), np.ones(dataNumFaultTest)), axis = -1)\n",
        "\n",
        "Y_train = np.vstack((trainLabelNormal, trainLabelFault))\n",
        "Y_test = np.vstack((testLabelNormal, testLabelFault))\n",
        "\n",
        "print('Y_train Normal:Fault = {:d}:{:d}'.format(len(trainLabelNormal), len(trainLabelFault)))\n",
        "print('Y_test  Normal:Fault = {:d}:{:d}'.format(len(testLabelNormal), len(testLabelFault)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train Normal:Fault = 800:800\n",
            "Y_test  Normal:Fault = 200:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kczwZk7Tbp5C",
        "colab_type": "code",
        "outputId": "9f69a3b2-4357-4510-a44b-43108b6bca3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# pretrainedModel = 'ResNet50'\n",
        "# lastActivation = 'softmax'\n",
        "# lossFunction = 'binary_crossentropy'\n",
        "# sizeBatch = 2\n",
        "# numEpochs = 2\n",
        "# verb = 1\n",
        "\n",
        "# Refresh all background variables\n",
        "K.clear_session()\n",
        "\n",
        "input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "# Building sequential model with name 'model'\n",
        "model = Sequential()\n",
        "\n",
        "# Model selection\n",
        "\n",
        "if (pretrainedModel == 'VGG16'):\n",
        "    \n",
        "    modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "elif (pretrainedModel == 'VGG19'):\n",
        "    \n",
        "    modelWoTop = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif pretrainedModel == 'ResNet50':\n",
        "    \n",
        "    modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif (pretrainedModel == 'InceptionV3'):\n",
        "    modelWoTop = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "elif (pretrainedModel == 'Xception'):\n",
        "    modelWoTop = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "                      \n",
        "elif (pretrainedModel == 'DenseNet169'):\n",
        "\n",
        "    modelWoTop = DenseNet169(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif (pretrainedModel == 'DenseNet201'):\n",
        "    modelWoTop = DenseNet201(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "else:\n",
        "    print('Invalid Pretrained Model Selection')\n",
        "    \n",
        "              \n",
        "\n",
        "# Model compiling\n",
        "\n",
        "print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "              \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Compiling Pretrained vgg16 Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eA9GZhLGe5X9",
        "colab_type": "code",
        "collapsed": true,
        "outputId": "fc5fef09-b963-4413-d7af-567d6861f9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "sizeBatch = 16\n",
        "\n",
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 16\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 76s 48ms/step - loss: 7.9509 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ltFYWEPxR9Oq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10837
        },
        "outputId": "a14ef585-818d-4ae3-cfdc-5076f85ae4c6"
      },
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "\n",
        "    sizeBatch = 16 + i\n",
        "\n",
        "    print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "    print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "    # Building sequential model with name 'model'\n",
        "    model = Sequential()\n",
        "\n",
        "    modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 16\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 7.8498 - acc: 0.5044 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 17\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 85s 53ms/step - loss: 8.0391 - acc: 0.4931 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 18\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 81s 51ms/step - loss: 7.9703 - acc: 0.4969 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 19\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 80s 50ms/step - loss: 7.9766 - acc: 0.4969 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 69s 43ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 20\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 77s 48ms/step - loss: 7.8334 - acc: 0.5069 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 21\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 76s 47ms/step - loss: 7.8882 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 66s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 22\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 74s 46ms/step - loss: 7.9556 - acc: 0.4987 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 23\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 76s 48ms/step - loss: 7.8010 - acc: 0.5081 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 24\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 45ms/step - loss: 7.9343 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 25\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 70s 44ms/step - loss: 7.8714 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 60s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 60s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 26\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 74s 46ms/step - loss: 7.9192 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 27\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 7.8078 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 28\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 68s 43ms/step - loss: 7.7790 - acc: 0.5019 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 29\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 68s 43ms/step - loss: 7.9194 - acc: 0.4962 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 30\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 7.9192 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 31\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 65s 40ms/step - loss: 7.7795 - acc: 0.5012 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 32\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 7.6717 - acc: 0.5144 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 33\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 45ms/step - loss: 7.6977 - acc: 0.5056 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 59s 37ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 34\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 7.6968 - acc: 0.5050 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 58s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 35\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 44ms/step - loss: 7.6954 - acc: 0.5031 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 57s 36ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 36\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 70s 44ms/step - loss: 7.8851 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 56s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 37\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 70s 44ms/step - loss: 7.7302 - acc: 0.5044 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 55s 35ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 38\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 7.8498 - acc: 0.5019 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 55s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 39\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 68s 43ms/step - loss: 7.7154 - acc: 0.5088 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 54s 34ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 54s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 54s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 54s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 40\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 66s 41ms/step - loss: 7.8568 - acc: 0.4956 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 53s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 41\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 7.7683 - acc: 0.5031 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 52s 33ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 42\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 66s 41ms/step - loss: 7.7844 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 52s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 43\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 66s 41ms/step - loss: 7.8924 - acc: 0.4925 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 51s 32ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 44\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 7.8418 - acc: 0.4994 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Training Pretrained vgg16 Model\n",
            "Batch Size: 45\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 64s 40ms/step - loss: 7.8200 - acc: 0.5019 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 50s 31ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FJxXBn2HVklR",
        "colab_type": "code",
        "outputId": "2e03eecd-dbbe-47af-f695-8cbb0d350142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=16, epochs=1, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/1\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b5347c438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "v1ezHQtSbcaK",
        "colab_type": "code",
        "outputId": "cc73f684-d724-46ba-d6ed-cba5a48a3ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=16, epochs=1, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/1\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b540c3da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "qexmaPZVz04q",
        "colab_type": "code",
        "outputId": "7d2a442d-46a4-481b-b736-fd3a5e10f095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(Y_test[:, 1], 'r')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(Y_pred[:, 1], 'b')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(Y_test[:, 1] - Y_pred[:, 1], 'g')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbQAALPH/AcfgYmnHLNk7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2b5c78bbe0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IPTFOJh3DTgq",
        "colab_type": "code",
        "outputId": "f530f03a-cd80-4fb8-cb9a-3d2237b6611a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "\n",
        "# modelSaved = '{}-{}_{}_{}.h5'.format(folderNormal, folderFault, pretrainedModel, now.strftime('%m-%d-%H:%M:%S'))\n",
        "\n",
        "modelSaved = '{}-{}_{}_{}.h5'.format('Normal', 'Fault', pretrainedModel, now.strftime('%m-%d-%H:%M:%S'))\n",
        "meanSaved = 'mean_{}.npy'.format(now.strftime('%m-%d-%H:%M:%S'))\n",
        "stdSaved = 'std_{}.npy'.format(now.strftime('%m-%d-%H:%M:%S'))\n",
        "\n",
        "inputStr = input('''Save Model as '{}'? (y/n)\\n'''.format(modelSaved))\n",
        "\n",
        "if (inputStr == 'y' or inputStr == 'Y'):  \n",
        "    model.save('gdrive/My Drive/Colab/Model/{}'.format(modelSaved))\n",
        "    np.save('gdrive/My Drive/Colab/Model/{}'.format(meanSaved), trainMean)\n",
        "    np.save('gdrive/My Drive/Colab/Model/{}'.format(stdSaved), trainStd)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Model as 'Normal-Fault_VGG16_10-18-03:55:54.h5'? (y/n)\n",
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}