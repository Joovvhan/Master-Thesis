{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning Clean Up.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joovvhan/Master-Thesis/blob/master/log/(PN)N-F_Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c3821KWLNey6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wf\n",
        "import time\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "# Import Keras modules\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxSASpgF7unA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.listdir('gdrive/My Drive/Colab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rw9NU8Xjwcz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set data directories\n",
        "\n",
        "dataPath = '''D:\\\\0_Joowhan's Paper\\\\Synthesized\\\\Total\\\\Impulse'''\n",
        "modelPath = '''D:\\\\0_Joowhan's Paper\\\\Synthesized\\\\Total\\\\Model'''\n",
        "\n",
        "\n",
        "# Changed variable names to normal and fault\n",
        "# Changed variable names from folder to path\n",
        "# Need to consider multiple folders\n",
        "# Need to add files to be tested\n",
        "\n",
        "# folderNormal = ['P1N1', 'P2N1', 'P3N1', 'P4N1', 'P5N1']\n",
        "folderNormal = list()\n",
        "\n",
        "for x in range(1, 6):\n",
        "    for y in range(1, 6):\n",
        "        if (x <= 2 and y <= 2):\n",
        "            folderNormal.append('P{}N{}'.format(x, y))\n",
        "\n",
        "folderFault = list()\n",
        "\n",
        "# folderFault = ['P1N5', 'P2N5', 'P3N5', 'P4N5', 'P5N5']\n",
        "\n",
        "for x in range(1, 6):\n",
        "    for y in range(1, 6):\n",
        "        if (x == 5 or y == 5):\n",
        "            folderFault.append('P{}N{}'.format(x, y))\n",
        "\n",
        "\n",
        "pathNormal = list()\n",
        "pathFault = list()\n",
        "\n",
        "for i in range(len(folderNormal)):\n",
        "    pathNormal.append(dataPath + '/' + folderNormal[i])\n",
        "    \n",
        "for i in range(len(folderFault)):\n",
        "    pathFault.append(dataPath + '/' + folderFault[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgGP7PZO-rTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specgram settings\n",
        "\n",
        "nsc = 1470\n",
        "nov = nsc/2\n",
        "nff = nsc \n",
        "imgSize = 224\n",
        "\n",
        "# pretrainedModel = 'VGG19'\n",
        "pretrainedModel = 'Xception'\n",
        "# pretrainedModel = 'ResNet50'\n",
        "\n",
        "# pretrainedModel = 'DenseNet169'\n",
        "# pretrainedModel = 'InceptionV3'\n",
        "\n",
        "\n",
        "lastActivation = 'softmax'\n",
        "sizeBatch = 4\n",
        "numEpochs = 12\n",
        "verb = 1\n",
        "\n",
        "\n",
        "# Learning parameters\n",
        "\n",
        "trainingRatio = 0.8\n",
        "\n",
        "totalDataNum = 1000\n",
        "repeat = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AskF2qFT2xr",
        "colab_type": "code",
        "outputId": "cdb01a36-a978-4833-dc88-3e608e9b6f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "startNum = 0\n",
        "\n",
        "pickNum = int(np.ceil(totalDataNum / len(pathNormal)))\n",
        "\n",
        "for i in range(startNum, len(pathNormal)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathNormal[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    pickIdx = np.random.choice(1000, pickNum, replace=False)\n",
        "    pickIdx.sort()\n",
        "    \n",
        "#     imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "#     label = data[:, 1:5]\n",
        "\n",
        "    imgs = np.moveaxis(np.dstack(data[pickIdx, 0]), 2, 0)\n",
        "    label = data[pickIdx, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelListNormal = label\n",
        "        imgsNormal = imgs\n",
        "    else:\n",
        "        labelListNormal = np.vstack([labelListNormal, label])\n",
        "        imgsNormal = np.vstack([imgsNormal, imgs])\n",
        "\n",
        "    print('Normal Image Shape From {}: {}:'.format(pathNormal[i], totalDataNum))    \n",
        "    print('Selected {}/{}:'.format(pickNum * (i + 1), len(data)))    \n",
        "    \n",
        "    \n",
        "print('Normal Image Shape: {}'.format(imgsNormal.shape))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P1N1: 1000:\n",
            "Selected 250/1000:\n",
            "Normal Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P1N2: 1000:\n",
            "Selected 500/1000:\n",
            "Normal Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P2N1: 1000:\n",
            "Selected 750/1000:\n",
            "Normal Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P2N2: 1000:\n",
            "Selected 1000/1000:\n",
            "Normal Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_MZtpk3Br_i",
        "colab_type": "code",
        "outputId": "70dba17d-8287-428b-b247-87e920b7f862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "startNum = 0\n",
        "\n",
        "pickNum = int(np.ceil(totalDataNum / len(pathFault)))\n",
        "\n",
        "for i in range(startNum, len(pathFault)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathFault[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    pickIdx = np.random.choice(1000, pickNum, replace=False)\n",
        "    pickIdx.sort()\n",
        "    \n",
        "#     imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "#     label = data[:, 1:5]\n",
        "    \n",
        "    imgs = np.moveaxis(np.dstack(data[pickIdx, 0]), 2, 0)\n",
        "    label = data[pickIdx, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelListFault = label\n",
        "        imgsFault = imgs\n",
        "    else:\n",
        "        labelListFault = np.vstack([labelListFault, label])\n",
        "        imgsFault = np.vstack([imgsFault, imgs])\n",
        "\n",
        "    print('Fault Image Shape From {}: {}:'.format(pathFault[i], totalDataNum))    \n",
        "    print('Selected {}/{}:'.format(pickNum * (i + 1), len(data)))     \n",
        "    \n",
        "print('Fault Image Shape: {}'.format(imgsFault.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P1N5: 1000:\n",
            "Selected 112/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P2N5: 1000:\n",
            "Selected 224/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P3N5: 1000:\n",
            "Selected 336/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P4N5: 1000:\n",
            "Selected 448/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P5N1: 1000:\n",
            "Selected 560/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P5N2: 1000:\n",
            "Selected 672/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P5N3: 1000:\n",
            "Selected 784/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P5N4: 1000:\n",
            "Selected 896/1000:\n",
            "Fault Image Shape From D:\\0_Joowhan's Paper\\Synthesized\\Total\\Impulse/P5N5: 1000:\n",
            "Selected 1008/1000:\n",
            "Fault Image Shape: (1008, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e91QsAurSkIP",
        "colab_type": "code",
        "outputId": "99152d3f-c7c5-4989-e81c-7b958fdd51c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "# Change name from imgsF1 or imgsF5 to imgsNormal and imgsFault\n",
        "\n",
        "dataNumNormal = len(imgsNormal)\n",
        "dataNumFault = len(imgsFault)\n",
        "dataNumNormalTrain = int(dataNumNormal * trainingRatio)\n",
        "dataNumFaultTrain = int(dataNumFault * trainingRatio)\n",
        "dataNumNormalTest = dataNumNormal - dataNumNormalTrain\n",
        "dataNumFaultTest = dataNumFault - dataNumFaultTrain\n",
        "\n",
        "print('Normal Train:Test = {:d}:{:d}'.format(dataNumNormalTrain, dataNumNormalTest))\n",
        "print('Fault  Train:Test = {:d}:{:d}\\n'.format(dataNumFaultTrain, dataNumFaultTest))\n",
        "\n",
        "trainIdxNormal = np.random.choice(dataNumNormal - 1, dataNumNormalTrain, replace=False)\n",
        "testIdxNormal = list(set(range(0, dataNumNormal)) - set(trainIdxNormal))\n",
        "\n",
        "trainImgsNormal = imgsNormal[trainIdxNormal, :, :]\n",
        "testImgsNormal = imgsNormal[testIdxNormal, :, :]\n",
        "\n",
        "print('Normal Training Image Shape {}'.format(trainImgsNormal.shape))\n",
        "print('Normal Test Image Shape {}\\n'.format(testImgsNormal.shape))\n",
        "\n",
        "trainIdxFault  = np.random.choice(dataNumFault - 1, dataNumFaultTrain, replace=False)\n",
        "testIdxFault = list(set(range(0, dataNumFault)) - set(trainIdxFault))\n",
        "\n",
        "trainImgsFault = imgsFault[trainIdxFault, :, :]\n",
        "testImgsFault = imgsFault[testIdxFault, :, :]\n",
        "\n",
        "print('Fault Training Image Shape {}'.format(trainImgsFault.shape))\n",
        "print('Fault Test Image Shape {}\\n'.format(testImgsFault.shape))\n",
        "\n",
        "trainImgs = np.vstack([trainImgsNormal, trainImgsFault])\n",
        "testImgs = np.vstack([testImgsNormal, testImgsFault])\n",
        "\n",
        "print('Training Image Shape {}'.format(trainImgs.shape))\n",
        "print('Test Image Shape {}'.format(testImgs.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Train:Test = 800:200\n",
            "Fault  Train:Test = 806:202\n",
            "\n",
            "Normal Training Image Shape (800, 224, 224)\n",
            "Normal Test Image Shape (200, 224, 224)\n",
            "\n",
            "Fault Training Image Shape (806, 224, 224)\n",
            "Fault Test Image Shape (202, 224, 224)\n",
            "\n",
            "Training Image Shape (1606, 224, 224)\n",
            "Test Image Shape (402, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MoE7f_hzs0av",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "feQ_AHAcUgHD",
        "colab_type": "code",
        "outputId": "8100874e-0e63-4ee5-8443-a031ba175bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainMean = np.mean(trainImgs)\n",
        "trainStd = np.std(trainImgs)\n",
        "\n",
        "print('Mean of Training Image: {}'.format(trainMean))\n",
        "print('Standard Deviation of Training Image: {}'.format(trainStd))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Training Image: -79.21006028451447\n",
            "Standard Deviation of Training Image: 8.117297388637008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wUi9IuXNUpir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Should Change Norm to Normalized\n",
        "\n",
        "trainImgsNorm = (trainImgs - trainMean) / trainStd\n",
        "testImgsNorm = (testImgs - trainMean) / trainStd\n",
        "\n",
        "trainImgsNorm = trainImgsNorm.reshape(list(trainImgsNorm.shape) + [1])\n",
        "testImgsNorm = testImgsNorm.reshape(list(testImgsNorm.shape) + [1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aklxiPz1Ve4n",
        "colab_type": "code",
        "outputId": "55e9f94d-26d4-49e0-c5ba-cbb8476addd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = np.stack([trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0]], axis = -1)\n",
        "X_test = np.stack([testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0]], axis = -1)\n",
        "\n",
        "print('X_train Shape: {}'.format(X_train.shape))\n",
        "print('X_test  Shape: {}'.format(X_test.shape))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train Shape: (1606, 224, 224, 3)\n",
            "X_test  Shape: (402, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRgIPzT7K_YP",
        "colab_type": "code",
        "outputId": "455af9bf-18bb-4282-acd3-5cb6e3deb343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainLabelNormal = np.stack((np.ones(dataNumNormalTrain), np.zeros(dataNumNormalTrain)), axis = -1)\n",
        "testLabelNormal = np.stack((np.ones(dataNumNormalTest), np.zeros(dataNumNormalTest)), axis = -1)\n",
        "\n",
        "trainLabelFault = np.stack((np.zeros(dataNumFaultTrain), np.ones(dataNumFaultTrain)), axis = -1)\n",
        "testLabelFault = np.stack((np.zeros(dataNumFaultTest), np.ones(dataNumFaultTest)), axis = -1)\n",
        "\n",
        "Y_train = np.vstack((trainLabelNormal, trainLabelFault))\n",
        "Y_test = np.vstack((testLabelNormal, testLabelFault))\n",
        "\n",
        "print('Y_train Normal:Fault = {:d}:{:d}'.format(len(trainLabelNormal), len(trainLabelFault)))\n",
        "print('Y_test  Normal:Fault = {:d}:{:d}'.format(len(testLabelNormal), len(testLabelFault)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train Normal:Fault = 800:806\n",
            "Y_test  Normal:Fault = 200:202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kczwZk7Tbp5C",
        "colab_type": "code",
        "outputId": "6df06439-6910-454a-97af-fa07b677c758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2647
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# pretrainedModel = 'ResNet50'\n",
        "# lastActivation = 'softmax'\n",
        "# lossFunction = 'binary_crossentropy'\n",
        "# sizeBatch = 2\n",
        "# numEpochs = 2\n",
        "# verb = 1\n",
        "\n",
        "for rp in range(repeat):\n",
        "\n",
        "    # Refresh all background variables\n",
        "    K.clear_session()\n",
        "\n",
        "    input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "    # Building sequential model with name 'model'\n",
        "    model = Sequential()\n",
        "\n",
        "    # Model selection\n",
        "\n",
        "    if (pretrainedModel == 'VGG16'):\n",
        "\n",
        "        modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'VGG19'):\n",
        "\n",
        "        modelWoTop = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif pretrainedModel == 'ResNet50':\n",
        "\n",
        "        modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'InceptionV3'):\n",
        "        modelWoTop = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'Xception'):\n",
        "        modelWoTop = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'DenseNet169'):\n",
        "\n",
        "        modelWoTop = DenseNet169(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'DenseNet201'):\n",
        "        modelWoTop = DenseNet201(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    else:\n",
        "        print('Invalid Pretrained Model Selection')\n",
        "\n",
        "\n",
        "\n",
        "    # Model compiling\n",
        "\n",
        "    print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "    print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "              validation_data=(X_test, Y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/12\n",
            "1606/1606 [==============================] - 84s 53ms/step - loss: 0.4752 - acc: 0.8064 - val_loss: 1.1549 - val_acc: 0.6716\n",
            "Epoch 2/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.4254 - acc: 0.8281 - val_loss: 2.8741 - val_acc: 0.6741\n",
            "Epoch 3/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.3140 - acc: 0.8917 - val_loss: 0.1001 - val_acc: 0.9776\n",
            "Epoch 4/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.2669 - acc: 0.9091 - val_loss: 2.5744 - val_acc: 0.4577\n",
            "Epoch 5/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.1257 - acc: 0.9589 - val_loss: 0.0693 - val_acc: 0.9876\n",
            "Epoch 6/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.1132 - acc: 0.9664 - val_loss: 1.3434 - val_acc: 0.8085\n",
            "Epoch 7/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.0947 - acc: 0.9701 - val_loss: 0.0656 - val_acc: 0.9751\n",
            "Epoch 8/12\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.0545 - acc: 0.9857 - val_loss: 0.0493 - val_acc: 0.9826\n",
            "Epoch 9/12\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0626 - acc: 0.9776 - val_loss: 0.0364 - val_acc: 0.9900\n",
            "Epoch 10/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0678 - acc: 0.9838 - val_loss: 0.0402 - val_acc: 0.9876\n",
            "Epoch 11/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0446 - acc: 0.9857 - val_loss: 0.0304 - val_acc: 0.9900\n",
            "Epoch 12/12\n",
            "1606/1606 [==============================] - 75s 46ms/step - loss: 0.0426 - acc: 0.9875 - val_loss: 0.0545 - val_acc: 0.9876\n",
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/12\n",
            "1606/1606 [==============================] - 85s 53ms/step - loss: 0.4812 - acc: 0.8151 - val_loss: 1.1207 - val_acc: 0.8632\n",
            "Epoch 2/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.3865 - acc: 0.8574 - val_loss: 4.1377 - val_acc: 0.5249\n",
            "Epoch 3/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.2559 - acc: 0.9022 - val_loss: 0.2827 - val_acc: 0.8930\n",
            "Epoch 4/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.2127 - acc: 0.9346 - val_loss: 0.0560 - val_acc: 0.9876\n",
            "Epoch 5/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.1173 - acc: 0.9651 - val_loss: 0.0521 - val_acc: 0.9876\n",
            "Epoch 6/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0973 - acc: 0.9732 - val_loss: 0.0417 - val_acc: 0.9876\n",
            "Epoch 7/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.1131 - acc: 0.9664 - val_loss: 6.2582 - val_acc: 0.5498\n",
            "Epoch 8/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.1291 - acc: 0.9583 - val_loss: 1.9882 - val_acc: 0.8134\n",
            "Epoch 9/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0479 - acc: 0.9863 - val_loss: 0.1232 - val_acc: 0.9602\n",
            "Epoch 10/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0435 - acc: 0.9888 - val_loss: 1.0143 - val_acc: 0.8905\n",
            "Epoch 11/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0780 - acc: 0.9801 - val_loss: 0.0411 - val_acc: 0.9876\n",
            "Epoch 12/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0754 - acc: 0.9770 - val_loss: 0.0901 - val_acc: 0.9876\n",
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/12\n",
            "1606/1606 [==============================] - 85s 53ms/step - loss: 0.4675 - acc: 0.8107 - val_loss: 0.3957 - val_acc: 0.8234\n",
            "Epoch 2/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.3379 - acc: 0.8923 - val_loss: 0.1509 - val_acc: 0.9776\n",
            "Epoch 3/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.2015 - acc: 0.9365 - val_loss: 2.1511 - val_acc: 0.7413\n",
            "Epoch 4/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.2522 - acc: 0.9110 - val_loss: 0.0890 - val_acc: 0.9876\n",
            "Epoch 5/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.1798 - acc: 0.9402 - val_loss: 2.1817 - val_acc: 0.6915\n",
            "Epoch 6/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.1053 - acc: 0.9714 - val_loss: 0.0867 - val_acc: 0.9876\n",
            "Epoch 7/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.1034 - acc: 0.9707 - val_loss: 0.1177 - val_acc: 0.9900\n",
            "Epoch 8/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0965 - acc: 0.9689 - val_loss: 0.1286 - val_acc: 0.9876\n",
            "Epoch 9/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0612 - acc: 0.9801 - val_loss: 0.0416 - val_acc: 0.9876\n",
            "Epoch 10/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0678 - acc: 0.9807 - val_loss: 0.0492 - val_acc: 0.9876\n",
            "Epoch 11/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0565 - acc: 0.9838 - val_loss: 0.0392 - val_acc: 0.9876\n",
            "Epoch 12/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.0661 - acc: 0.9807 - val_loss: 0.0976 - val_acc: 0.9851\n",
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/12\n",
            "1606/1606 [==============================] - 85s 53ms/step - loss: 0.4362 - acc: 0.8375 - val_loss: 4.0256 - val_acc: 0.4975\n",
            "Epoch 2/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.2569 - acc: 0.9110 - val_loss: 0.0656 - val_acc: 0.9876\n",
            "Epoch 3/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.1934 - acc: 0.9371 - val_loss: 0.0707 - val_acc: 0.9876\n",
            "Epoch 4/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.1488 - acc: 0.9564 - val_loss: 0.0542 - val_acc: 0.9876\n",
            "Epoch 5/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0999 - acc: 0.9707 - val_loss: 0.4601 - val_acc: 0.8930\n",
            "Epoch 6/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0965 - acc: 0.9695 - val_loss: 0.0403 - val_acc: 0.9876\n",
            "Epoch 7/12\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0420 - acc: 0.9869 - val_loss: 0.0435 - val_acc: 0.9876\n",
            "Epoch 8/12\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0593 - acc: 0.9807 - val_loss: 5.2266 - val_acc: 0.5746\n",
            "Epoch 9/12\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0926 - acc: 0.9701 - val_loss: 0.0394 - val_acc: 0.9876\n",
            "Epoch 10/12\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0479 - acc: 0.9863 - val_loss: 0.0382 - val_acc: 0.9900\n",
            "Epoch 11/12\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.1109 - acc: 0.9658 - val_loss: 0.0465 - val_acc: 0.9876\n",
            "Epoch 12/12\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0596 - acc: 0.9838 - val_loss: 0.0383 - val_acc: 0.9876\n",
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/12\n",
            "1606/1606 [==============================] - 84s 52ms/step - loss: 0.4366 - acc: 0.8344 - val_loss: 0.0846 - val_acc: 0.9751\n",
            "Epoch 2/12\n",
            "1606/1606 [==============================] - 76s 47ms/step - loss: 0.4254 - acc: 0.8176 - val_loss: 1.2956 - val_acc: 0.7711\n",
            "Epoch 3/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.3720 - acc: 0.8655 - val_loss: 0.1001 - val_acc: 0.9328\n",
            "Epoch 4/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.2753 - acc: 0.9047 - val_loss: 0.3792 - val_acc: 0.7313\n",
            "Epoch 5/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.1280 - acc: 0.9595 - val_loss: 0.0509 - val_acc: 0.9876\n",
            "Epoch 6/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0933 - acc: 0.9676 - val_loss: 0.0492 - val_acc: 0.9876\n",
            "Epoch 7/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.1147 - acc: 0.9664 - val_loss: 0.0449 - val_acc: 0.9876\n",
            "Epoch 8/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0586 - acc: 0.9857 - val_loss: 0.1394 - val_acc: 0.9652\n",
            "Epoch 9/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0846 - acc: 0.9782 - val_loss: 0.0647 - val_acc: 0.9876\n",
            "Epoch 10/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0502 - acc: 0.9863 - val_loss: 0.0384 - val_acc: 0.9876\n",
            "Epoch 11/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0297 - acc: 0.9925 - val_loss: 0.0771 - val_acc: 0.9851\n",
            "Epoch 12/12\n",
            "1606/1606 [==============================] - 75s 47ms/step - loss: 0.0609 - acc: 0.9813 - val_loss: 0.0547 - val_acc: 0.9876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LMf390ePDy2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "70c14f2e-3c8b-491e-e6c1-78925a66404d"
      },
      "cell_type": "code",
      "source": [
        "# Refresh all background variables\n",
        "K.clear_session()\n",
        "\n",
        "input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "# Building sequential model with name 'model'\n",
        "model = Sequential()\n",
        "\n",
        "# Model selection\n",
        "\n",
        "if (pretrainedModel == 'VGG16'):\n",
        "\n",
        "    modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif (pretrainedModel == 'VGG19'):\n",
        "\n",
        "    modelWoTop = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif pretrainedModel == 'ResNet50':\n",
        "\n",
        "    modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif (pretrainedModel == 'InceptionV3'):\n",
        "    modelWoTop = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif (pretrainedModel == 'Xception'):\n",
        "    modelWoTop = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif (pretrainedModel == 'DenseNet169'):\n",
        "\n",
        "    modelWoTop = DenseNet169(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "elif (pretrainedModel == 'DenseNet201'):\n",
        "    modelWoTop = DenseNet201(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "else:\n",
        "    print('Invalid Pretrained Model Selection')\n",
        "\n",
        "\n",
        "\n",
        "# Model compiling\n",
        "\n",
        "print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "numEpochs = 20\n",
        "\n",
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling Pretrained xception Model\n",
            "Training Pretrained xception Model\n",
            "Batch Size: 4\t Epochs: 20\t\n",
            "\n",
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/20\n",
            "1606/1606 [==============================] - 83s 52ms/step - loss: 0.4618 - acc: 0.8269 - val_loss: 8.0550 - val_acc: 0.4975\n",
            "Epoch 2/20\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.3017 - acc: 0.9103 - val_loss: 0.0658 - val_acc: 0.9851\n",
            "Epoch 3/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.2193 - acc: 0.9265 - val_loss: 0.0581 - val_acc: 0.9801\n",
            "Epoch 4/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.1180 - acc: 0.9626 - val_loss: 0.0736 - val_acc: 0.9876\n",
            "Epoch 5/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0944 - acc: 0.9682 - val_loss: 0.0591 - val_acc: 0.9851\n",
            "Epoch 6/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0801 - acc: 0.9770 - val_loss: 0.1081 - val_acc: 0.9851\n",
            "Epoch 7/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0699 - acc: 0.9826 - val_loss: 0.1013 - val_acc: 0.9627\n",
            "Epoch 8/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0626 - acc: 0.9795 - val_loss: 0.0425 - val_acc: 0.9851\n",
            "Epoch 9/20\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0498 - acc: 0.9863 - val_loss: 0.9070 - val_acc: 0.9104\n",
            "Epoch 10/20\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0988 - acc: 0.9751 - val_loss: 0.0723 - val_acc: 0.9876\n",
            "Epoch 11/20\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0431 - acc: 0.9875 - val_loss: 0.6044 - val_acc: 0.7786\n",
            "Epoch 12/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0633 - acc: 0.9813 - val_loss: 0.5280 - val_acc: 0.8905\n",
            "Epoch 13/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0417 - acc: 0.9857 - val_loss: 0.1450 - val_acc: 0.9701\n",
            "Epoch 14/20\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0669 - acc: 0.9801 - val_loss: 0.0449 - val_acc: 0.9851\n",
            "Epoch 15/20\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0628 - acc: 0.9819 - val_loss: 0.0367 - val_acc: 0.9876\n",
            "Epoch 16/20\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0836 - acc: 0.9757 - val_loss: 0.0350 - val_acc: 0.9876\n",
            "Epoch 17/20\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0311 - acc: 0.9894 - val_loss: 0.0343 - val_acc: 0.9925\n",
            "Epoch 18/20\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0315 - acc: 0.9888 - val_loss: 0.0766 - val_acc: 0.9751\n",
            "Epoch 19/20\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0392 - acc: 0.9919 - val_loss: 0.0354 - val_acc: 0.9900\n",
            "Epoch 20/20\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0362 - val_acc: 0.9900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1fac1a4b748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "jKgFIa4gkgkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "da713a28-6da4-4b3e-f893-9e5de627be5f"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=4, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/4\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.0169 - acc: 0.9963 - val_loss: 0.0387 - val_acc: 0.9900\n",
            "Epoch 2/4\n",
            "1606/1606 [==============================] - 73s 45ms/step - loss: 0.0431 - acc: 0.9851 - val_loss: 0.0681 - val_acc: 0.9876\n",
            "Epoch 3/4\n",
            "1606/1606 [==============================] - 72s 45ms/step - loss: 0.0286 - acc: 0.9913 - val_loss: 0.0349 - val_acc: 0.9900\n",
            "Epoch 4/4\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0443 - acc: 0.9869 - val_loss: 2.2373 - val_acc: 0.8358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1faa087cd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "wIlomC_jUUhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "a3869484-abf8-4a43-93b2-1d9ae794180e"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=4, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1606 samples, validate on 402 samples\n",
            "Epoch 1/4\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0341 - acc: 0.9900 - val_loss: 0.0286 - val_acc: 0.9925\n",
            "Epoch 2/4\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0265 - acc: 0.9944 - val_loss: 0.0517 - val_acc: 0.9900\n",
            "Epoch 3/4\n",
            "1606/1606 [==============================] - 74s 46ms/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.0564 - val_acc: 0.9900\n",
            "Epoch 4/4\n",
            "1606/1606 [==============================] - 73s 46ms/step - loss: 0.0122 - acc: 0.9975 - val_loss: 0.0696 - val_acc: 0.9876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1faa087c8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "agqqH8sf3CiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# K.clear_session()\n",
        "\n",
        "# input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "# # Building sequential model with name 'model'\n",
        "# model = Sequential()\n",
        "\n",
        "# modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "# model.add(modelWoTop)\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "# # Model compiling\n",
        "\n",
        "# print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# numEpochs = 1\n",
        "\n",
        "# print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "# print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "# model.fit(X_train, Y_train,\n",
        "#           batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "#           validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qexmaPZVz04q",
        "colab_type": "code",
        "outputId": "d48b8251-f306-43fb-95df-8ed230d913ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(Y_test[:, 1], 'r')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(Y_pred[:, 1], 'b')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(Y_test[:, 1] - Y_pred[:, 1], 'g')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IPTFOJh3DTgq",
        "colab_type": "code",
        "outputId": "a2e67208-01fb-4f1c-ae8b-15fd808fbd95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "\n",
        "# modelSaved = '{}-{}_{}_{}.h5'.format(folderNormal, folderFault, pretrainedModel, now.strftime('%m-%d-%H:%M:%S'))\n",
        "\n",
        "modelSaved = '(PN){}-{}_{}_{}.h5'.format('N', 'F', pretrainedModel, now.strftime('%m-%d-%H-%M-%S'))\n",
        "meanSaved = 'mean_{}.npy'.format(now.strftime('%m-%d-%H-%M-%S'))\n",
        "stdSaved = 'std_{}.npy'.format(now.strftime('%m-%d-%H-%M-%S'))\n",
        "\n",
        "inputStr = input('''Save Model as '{}'? (y/n)\\n'''.format(modelSaved))\n",
        "\n",
        "if (inputStr == 'y' or inputStr == 'Y'):  \n",
        "    model.save(modelPath + '/{}'.format(modelSaved))\n",
        "    np.save(modelPath + '/{}'.format(meanSaved), trainMean)\n",
        "    np.save(modelPath + '/{}'.format(stdSaved), trainStd)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Model as '(PN)N-F_Xception_11-19-18-39-51.h5'? (y/n)\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}