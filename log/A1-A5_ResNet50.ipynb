{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning Clean Up.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joovvhan/Master-Thesis/blob/master/log/A1-A5_ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c3821KWLNey6",
        "colab_type": "code",
        "outputId": "a9a4f02d-3e3e-4ee3-aec3-b9f5e592f480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wf\n",
        "import time\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "# Import Keras modules\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nxSASpgF7unA",
        "colab_type": "code",
        "outputId": "34dd39f0-bcb0-4948-ff5f-d181111752e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.listdir('gdrive/My Drive/Colab')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data', 'Model', 'Data_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "rw9NU8Xjwcz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set data directories\n",
        "\n",
        "dataPath = 'gdrive/My Drive/Colab/Data'\n",
        "\n",
        "# Changed variable names to normal and fault\n",
        "# Changed variable names from folder to path\n",
        "# Need to consider multiple folders\n",
        "# Need to add files to be tested\n",
        "\n",
        "#folderNormal = ['A1F3P3']\n",
        "folderNormal = list()\n",
        "\n",
        "for x in range(1, 2):\n",
        "    for y in range(1, 6):\n",
        "        for z in range(1, 6):\n",
        "            #if (x <= 2 and y <= 2 and z <= 2):\n",
        "            folderNormal.append('A{}F{}P{}'.format(x, y, z))\n",
        "\n",
        "folderFault = list()\n",
        "\n",
        "# folderFault = ['A5F3P3']\n",
        "\n",
        "for x in range(5, 6):\n",
        "    for y in range(1, 6):\n",
        "        for z in range(1, 6):\n",
        "            #if (x == 5 or y == 5 or z == 5):\n",
        "            folderFault.append('A{}F{}P{}'.format(x, y, z))\n",
        "\n",
        "\n",
        "pathNormal = list()\n",
        "pathFault = list()\n",
        "\n",
        "for i in range(len(folderNormal)):\n",
        "    pathNormal.append(dataPath + '/' + folderNormal[i])\n",
        "    \n",
        "for i in range(len(folderFault)):\n",
        "    pathFault.append(dataPath + '/' + folderFault[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgGP7PZO-rTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specgram settings\n",
        "\n",
        "nsc = 1470\n",
        "nov = nsc/2\n",
        "nff = nsc \n",
        "imgSize = 224\n",
        "\n",
        "# pretrainedModel = 'VGG19'\n",
        "# pretrainedModel = 'Xception'\n",
        "pretrainedModel = 'ResNet50'\n",
        "\n",
        "# pretrainedModel = 'VGG19'\n",
        "# pretrainedModel = 'InceptionV3'\n",
        "#pretrainedModel = 'DenseNet169'\n",
        "#pretrainedModel = 'DenseNet201'\n",
        "\n",
        "\n",
        "lastActivation = 'softmax'\n",
        "sizeBatch = 4\n",
        "numEpochs = 12\n",
        "verb = 1\n",
        "\n",
        "\n",
        "# Learning parameters\n",
        "\n",
        "trainingRatio = 0.8\n",
        "\n",
        "totalDataNum = 1000\n",
        "repeat = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AskF2qFT2xr",
        "colab_type": "code",
        "outputId": "1ddc3587-2e5c-484e-f377-3772e9993220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "cell_type": "code",
      "source": [
        "startNum = 0\n",
        "\n",
        "pickNum = int(np.ceil(totalDataNum / len(pathNormal)))\n",
        "\n",
        "for i in range(startNum, len(pathNormal)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathNormal[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    pickIdx = np.random.choice(1000, pickNum, replace=False)\n",
        "    pickIdx.sort()\n",
        "    \n",
        "#     imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "#     label = data[:, 1:5]\n",
        "\n",
        "    imgs = np.moveaxis(np.dstack(data[pickIdx, 0]), 2, 0)\n",
        "    label = data[pickIdx, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelListNormal = label\n",
        "        imgsNormal = imgs\n",
        "    else:\n",
        "        labelListNormal = np.vstack([labelListNormal, label])\n",
        "        imgsNormal = np.vstack([imgsNormal, imgs])\n",
        "\n",
        "    print('Normal Image Shape From {}: {}:'.format(pathNormal[i], totalDataNum))    \n",
        "    print('Selected {}/{}:'.format(pickNum * (i + 1), len(data)))    \n",
        "    \n",
        "    \n",
        "print('Normal Image Shape: {}'.format(imgsNormal.shape))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F1P1: 1000:\n",
            "Selected 40/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F1P2: 1000:\n",
            "Selected 80/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F1P3: 1000:\n",
            "Selected 120/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F1P4: 1000:\n",
            "Selected 160/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F1P5: 1000:\n",
            "Selected 200/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F2P1: 1000:\n",
            "Selected 240/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F2P2: 1000:\n",
            "Selected 280/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F2P3: 1000:\n",
            "Selected 320/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F2P4: 1000:\n",
            "Selected 360/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F2P5: 1000:\n",
            "Selected 400/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F3P1: 1000:\n",
            "Selected 440/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F3P2: 1000:\n",
            "Selected 480/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F3P3: 1000:\n",
            "Selected 520/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F3P4: 1000:\n",
            "Selected 560/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F3P5: 1000:\n",
            "Selected 600/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F4P1: 1000:\n",
            "Selected 640/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F4P2: 1000:\n",
            "Selected 680/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F4P3: 1000:\n",
            "Selected 720/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F4P4: 1000:\n",
            "Selected 760/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F4P5: 1000:\n",
            "Selected 800/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F5P1: 1000:\n",
            "Selected 840/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F5P2: 1000:\n",
            "Selected 880/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F5P3: 1000:\n",
            "Selected 920/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F5P4: 1000:\n",
            "Selected 960/1000:\n",
            "Normal Image Shape From gdrive/My Drive/Colab/Data/A1F5P5: 1000:\n",
            "Selected 1000/1000:\n",
            "Normal Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_MZtpk3Br_i",
        "colab_type": "code",
        "outputId": "2649ecce-4d46-4f2f-a849-82d0d3e36379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "cell_type": "code",
      "source": [
        "startNum = 0\n",
        "\n",
        "pickNum = int(np.ceil(totalDataNum / len(pathFault)))\n",
        "\n",
        "for i in range(startNum, len(pathFault)):\n",
        "\n",
        "    npyTestPath = glob.glob(pathFault[i] + '/' + '*Image_With_Label.npy')\n",
        "    data = np.load(npyTestPath[0])\n",
        "    \n",
        "    pickIdx = np.random.choice(1000, pickNum, replace=False)\n",
        "    pickIdx.sort()\n",
        "    \n",
        "#     imgs = np.moveaxis(np.dstack(data[:, 0]), 2, 0)\n",
        "#     label = data[:, 1:5]\n",
        "    \n",
        "    imgs = np.moveaxis(np.dstack(data[pickIdx, 0]), 2, 0)\n",
        "    label = data[pickIdx, 1:5]\n",
        "    \n",
        "    if i == startNum:\n",
        "        labelListFault = label\n",
        "        imgsFault = imgs\n",
        "    else:\n",
        "        labelListFault = np.vstack([labelListFault, label])\n",
        "        imgsFault = np.vstack([imgsFault, imgs])\n",
        "\n",
        "    print('Fault Image Shape From {}: {}:'.format(pathFault[i], totalDataNum))    \n",
        "    print('Selected {}/{}:'.format(pickNum * (i + 1), len(data)))     \n",
        "    \n",
        "print('Fault Image Shape: {}'.format(imgsFault.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F1P1: 1000:\n",
            "Selected 40/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F1P2: 1000:\n",
            "Selected 80/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F1P3: 1000:\n",
            "Selected 120/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F1P4: 1000:\n",
            "Selected 160/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F1P5: 1000:\n",
            "Selected 200/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F2P1: 1000:\n",
            "Selected 240/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F2P2: 1000:\n",
            "Selected 280/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F2P3: 1000:\n",
            "Selected 320/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F2P4: 1000:\n",
            "Selected 360/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F2P5: 1000:\n",
            "Selected 400/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F3P1: 1000:\n",
            "Selected 440/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F3P2: 1000:\n",
            "Selected 480/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F3P3: 1000:\n",
            "Selected 520/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F3P4: 1000:\n",
            "Selected 560/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F3P5: 1000:\n",
            "Selected 600/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F4P1: 1000:\n",
            "Selected 640/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F4P2: 1000:\n",
            "Selected 680/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F4P3: 1000:\n",
            "Selected 720/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F4P4: 1000:\n",
            "Selected 760/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F4P5: 1000:\n",
            "Selected 800/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F5P1: 1000:\n",
            "Selected 840/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F5P2: 1000:\n",
            "Selected 880/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F5P3: 1000:\n",
            "Selected 920/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F5P4: 1000:\n",
            "Selected 960/1000:\n",
            "Fault Image Shape From gdrive/My Drive/Colab/Data/A5F5P5: 1000:\n",
            "Selected 1000/1000:\n",
            "Fault Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e91QsAurSkIP",
        "colab_type": "code",
        "outputId": "238f3b48-e754-480e-8aba-fce1de6ee164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "# Change name from imgsF1 or imgsF5 to imgsNormal and imgsFault\n",
        "\n",
        "dataNumNormal = len(imgsNormal)\n",
        "dataNumFault = len(imgsFault)\n",
        "dataNumNormalTrain = int(dataNumNormal * trainingRatio)\n",
        "dataNumFaultTrain = int(dataNumFault * trainingRatio)\n",
        "dataNumNormalTest = dataNumNormal - dataNumNormalTrain\n",
        "dataNumFaultTest = dataNumFault - dataNumFaultTrain\n",
        "\n",
        "print('Normal Train:Test = {:d}:{:d}'.format(dataNumNormalTrain, dataNumNormalTest))\n",
        "print('Fault  Train:Test = {:d}:{:d}\\n'.format(dataNumFaultTrain, dataNumFaultTest))\n",
        "\n",
        "trainIdxNormal = np.random.choice(dataNumNormal - 1, dataNumNormalTrain, replace=False)\n",
        "testIdxNormal = list(set(range(0, dataNumNormal)) - set(trainIdxNormal))\n",
        "\n",
        "trainImgsNormal = imgsNormal[trainIdxNormal, :, :]\n",
        "testImgsNormal = imgsNormal[testIdxNormal, :, :]\n",
        "\n",
        "print('Normal Training Image Shape {}'.format(trainImgsNormal.shape))\n",
        "print('Normal Test Image Shape {}\\n'.format(testImgsNormal.shape))\n",
        "\n",
        "trainIdxFault  = np.random.choice(dataNumFault - 1, dataNumFaultTrain, replace=False)\n",
        "testIdxFault = list(set(range(0, dataNumFault)) - set(trainIdxFault))\n",
        "\n",
        "trainImgsFault = imgsFault[trainIdxFault, :, :]\n",
        "testImgsFault = imgsFault[testIdxFault, :, :]\n",
        "\n",
        "print('Fault Training Image Shape {}'.format(trainImgsFault.shape))\n",
        "print('Fault Test Image Shape {}\\n'.format(testImgsFault.shape))\n",
        "\n",
        "trainImgs = np.vstack([trainImgsNormal, trainImgsFault])\n",
        "testImgs = np.vstack([testImgsNormal, testImgsFault])\n",
        "\n",
        "print('Training Image Shape {}'.format(trainImgs.shape))\n",
        "print('Test Image Shape {}'.format(testImgs.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Train:Test = 800:200\n",
            "Fault  Train:Test = 800:200\n",
            "\n",
            "Normal Training Image Shape (800, 224, 224)\n",
            "Normal Test Image Shape (200, 224, 224)\n",
            "\n",
            "Fault Training Image Shape (800, 224, 224)\n",
            "Fault Test Image Shape (200, 224, 224)\n",
            "\n",
            "Training Image Shape (1600, 224, 224)\n",
            "Test Image Shape (400, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MoE7f_hzs0av",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "feQ_AHAcUgHD",
        "colab_type": "code",
        "outputId": "92985f86-c355-4a2c-8fbc-35f84e0dc9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainMean = np.mean(trainImgs)\n",
        "trainStd = np.std(trainImgs)\n",
        "\n",
        "print('Mean of Training Image: {}'.format(trainMean))\n",
        "print('Standard Deviation of Training Image: {}'.format(trainStd))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Training Image: -78.54276589645568\n",
            "Standard Deviation of Training Image: 9.262852159993479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wUi9IuXNUpir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Should Change Norm to Normalized\n",
        "\n",
        "trainImgsNorm = (trainImgs - trainMean) / trainStd\n",
        "testImgsNorm = (testImgs - trainMean) / trainStd\n",
        "\n",
        "trainImgsNorm = trainImgsNorm.reshape(list(trainImgsNorm.shape) + [1])\n",
        "testImgsNorm = testImgsNorm.reshape(list(testImgsNorm.shape) + [1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aklxiPz1Ve4n",
        "colab_type": "code",
        "outputId": "021cf8da-8769-4e79-9680-4fc79d9696b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = np.stack([trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0]], axis = -1)\n",
        "X_test = np.stack([testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0]], axis = -1)\n",
        "\n",
        "print('X_train Shape: {}'.format(X_train.shape))\n",
        "print('X_test  Shape: {}'.format(X_test.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train Shape: (1600, 224, 224, 3)\n",
            "X_test  Shape: (400, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRgIPzT7K_YP",
        "colab_type": "code",
        "outputId": "3accbd19-48cb-4ca1-deee-c853406338e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "trainLabelNormal = np.stack((np.ones(dataNumNormalTrain), np.zeros(dataNumNormalTrain)), axis = -1)\n",
        "testLabelNormal = np.stack((np.ones(dataNumNormalTest), np.zeros(dataNumNormalTest)), axis = -1)\n",
        "\n",
        "trainLabelFault = np.stack((np.zeros(dataNumFaultTrain), np.ones(dataNumFaultTrain)), axis = -1)\n",
        "testLabelFault = np.stack((np.zeros(dataNumFaultTest), np.ones(dataNumFaultTest)), axis = -1)\n",
        "\n",
        "Y_train = np.vstack((trainLabelNormal, trainLabelFault))\n",
        "Y_test = np.vstack((testLabelNormal, testLabelFault))\n",
        "\n",
        "print('Y_train Normal:Fault = {:d}:{:d}'.format(len(trainLabelNormal), len(trainLabelFault)))\n",
        "print('Y_test  Normal:Fault = {:d}:{:d}'.format(len(testLabelNormal), len(testLabelFault)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train Normal:Fault = 800:800\n",
            "Y_test  Normal:Fault = 200:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kczwZk7Tbp5C",
        "colab_type": "code",
        "outputId": "fae69733-daae-4ba3-e54d-ce4ae55f4ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2719
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# pretrainedModel = 'ResNet50'\n",
        "# lastActivation = 'softmax'\n",
        "# lossFunction = 'binary_crossentropy'\n",
        "# sizeBatch = 2\n",
        "# numEpochs = 2\n",
        "# verb = 1\n",
        "\n",
        "for rp in range(repeat):\n",
        "\n",
        "    # Refresh all background variables\n",
        "    K.clear_session()\n",
        "\n",
        "    input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "    # Building sequential model with name 'model'\n",
        "    model = Sequential()\n",
        "\n",
        "    # Model selection\n",
        "\n",
        "    if (pretrainedModel == 'VGG16'):\n",
        "\n",
        "        modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'VGG19'):\n",
        "\n",
        "        modelWoTop = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(4096, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif pretrainedModel == 'ResNet50':\n",
        "\n",
        "        modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'InceptionV3'):\n",
        "        modelWoTop = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'Xception'):\n",
        "        modelWoTop = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'DenseNet169'):\n",
        "\n",
        "        modelWoTop = DenseNet169(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    elif (pretrainedModel == 'DenseNet201'):\n",
        "        modelWoTop = DenseNet201(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "        model.add(modelWoTop)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(2, activation=lastActivation))\n",
        "\n",
        "    else:\n",
        "        print('Invalid Pretrained Model Selection')\n",
        "\n",
        "\n",
        "\n",
        "    # Model compiling\n",
        "\n",
        "    print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "    print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "              validation_data=(X_test, Y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 14s 0us/step\n",
            "Compiling Pretrained resnet50 Model\n",
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "1600/1600 [==============================] - 138s 86ms/step - loss: 1.0754 - acc: 0.8969 - val_loss: 0.2514 - val_acc: 0.9975\n",
            "Epoch 2/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.6011 - acc: 0.9287 - val_loss: 0.1649 - val_acc: 1.0000\n",
            "Epoch 3/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.5397 - acc: 0.9419 - val_loss: 0.1766 - val_acc: 1.0000\n",
            "Epoch 4/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4780 - acc: 0.9494 - val_loss: 0.1333 - val_acc: 0.9650\n",
            "Epoch 5/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4553 - acc: 0.9469 - val_loss: 0.0521 - val_acc: 1.0000\n",
            "Epoch 6/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.6317 - acc: 0.9219 - val_loss: 1.7435 - val_acc: 0.8550\n",
            "Epoch 7/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.7127 - acc: 0.9206 - val_loss: 3.2923 - val_acc: 0.7825\n",
            "Epoch 8/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.5050 - acc: 0.9337 - val_loss: 0.0468 - val_acc: 1.0000\n",
            "Epoch 9/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4650 - acc: 0.9500 - val_loss: 0.1127 - val_acc: 0.9950\n",
            "Epoch 10/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4007 - acc: 0.9537 - val_loss: 0.0389 - val_acc: 0.9975\n",
            "Epoch 11/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.3510 - acc: 0.9563 - val_loss: 0.0305 - val_acc: 1.0000\n",
            "Epoch 12/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.3613 - acc: 0.9712 - val_loss: 0.0241 - val_acc: 1.0000\n",
            "Compiling Pretrained resnet50 Model\n",
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "1600/1600 [==============================] - 135s 84ms/step - loss: 8.0005 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/12\n",
            "1600/1600 [==============================] - 122s 77ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 9/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 10/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 11/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 12/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Compiling Pretrained resnet50 Model\n",
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "1600/1600 [==============================] - 135s 84ms/step - loss: 2.1749 - acc: 0.7981 - val_loss: 0.0278 - val_acc: 1.0000\n",
            "Epoch 2/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 2.3864 - acc: 0.8006 - val_loss: 8.0239 - val_acc: 0.5000\n",
            "Epoch 3/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 3.4662 - acc: 0.7700 - val_loss: 4.9275 - val_acc: 0.6925\n",
            "Epoch 4/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 2.6450 - acc: 0.8075 - val_loss: 7.5161 - val_acc: 0.5250\n",
            "Epoch 5/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 3.2869 - acc: 0.7831 - val_loss: 7.6527 - val_acc: 0.5000\n",
            "Epoch 6/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 3.0972 - acc: 0.7881 - val_loss: 6.3409 - val_acc: 0.5925\n",
            "Epoch 7/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 3.3757 - acc: 0.7806 - val_loss: 1.2398 - val_acc: 0.9150\n",
            "Epoch 8/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 1.7984 - acc: 0.8825 - val_loss: 6.9653 - val_acc: 0.5650\n",
            "Epoch 9/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.5560 - acc: 0.9575 - val_loss: 0.0026 - val_acc: 0.9975\n",
            "Epoch 10/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4410 - acc: 0.9656 - val_loss: 0.0689 - val_acc: 0.9925\n",
            "Epoch 11/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.5230 - acc: 0.9619 - val_loss: 0.1223 - val_acc: 0.9925\n",
            "Epoch 12/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.5343 - acc: 0.9669 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Compiling Pretrained resnet50 Model\n",
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "1600/1600 [==============================] - 135s 85ms/step - loss: 0.9492 - acc: 0.9125 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.7010 - acc: 0.9494 - val_loss: 2.1184 - val_acc: 0.8650\n",
            "Epoch 3/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3894 - acc: 0.9694 - val_loss: 6.1402e-04 - val_acc: 1.0000\n",
            "Epoch 4/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3875 - acc: 0.9725 - val_loss: 0.6136 - val_acc: 0.9600\n",
            "Epoch 5/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.4213 - acc: 0.9700 - val_loss: 9.3368e-05 - val_acc: 1.0000\n",
            "Epoch 6/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3559 - acc: 0.9744 - val_loss: 2.3391e-04 - val_acc: 1.0000\n",
            "Epoch 7/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3198 - acc: 0.9794 - val_loss: 1.9710 - val_acc: 0.8625\n",
            "Epoch 8/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3804 - acc: 0.9731 - val_loss: 0.0217 - val_acc: 0.9950\n",
            "Epoch 9/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.2909 - acc: 0.9819 - val_loss: 1.4832e-05 - val_acc: 1.0000\n",
            "Epoch 10/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.4020 - acc: 0.9731 - val_loss: 0.0045 - val_acc: 1.0000\n",
            "Epoch 11/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3944 - acc: 0.9738 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 12/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.8766 - acc: 0.9388 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Compiling Pretrained resnet50 Model\n",
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 12\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "1600/1600 [==============================] - 136s 85ms/step - loss: 2.2344 - acc: 0.8200 - val_loss: 8.0369 - val_acc: 0.4950\n",
            "Epoch 2/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 2.0070 - acc: 0.8675 - val_loss: 0.9843 - val_acc: 0.9325\n",
            "Epoch 3/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 3.2919 - acc: 0.7831 - val_loss: 3.3988 - val_acc: 0.7825\n",
            "Epoch 4/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 3.4518 - acc: 0.7788 - val_loss: 0.0931 - val_acc: 0.9900\n",
            "Epoch 5/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.9819 - acc: 0.9306 - val_loss: 0.2980 - val_acc: 0.9800\n",
            "Epoch 6/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 1.1528 - acc: 0.9144 - val_loss: 2.2903 - val_acc: 0.8450\n",
            "Epoch 7/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 2.1040 - acc: 0.8544 - val_loss: 3.0704 - val_acc: 0.8075\n",
            "Epoch 8/12\n",
            "1600/1600 [==============================] - 121s 76ms/step - loss: 0.4076 - acc: 0.9694 - val_loss: 2.9342e-04 - val_acc: 1.0000\n",
            "Epoch 9/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.2417 - acc: 0.9850 - val_loss: 1.4973e-04 - val_acc: 1.0000\n",
            "Epoch 10/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3794 - acc: 0.9706 - val_loss: 0.0406 - val_acc: 0.9975\n",
            "Epoch 11/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.4163 - acc: 0.9725 - val_loss: 2.4512e-04 - val_acc: 1.0000\n",
            "Epoch 12/12\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.5038 - acc: 0.9669 - val_loss: 0.1623 - val_acc: 0.9850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eA9GZhLGe5X9",
        "colab_type": "code",
        "collapsed": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "# print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "# model.fit(X_train, Y_train,\n",
        "#           batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "#           validation_data=(X_test, Y_test))\n",
        "\n",
        "# Y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSMpuzW6uljh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "87b496b8-719c-452a-870e-da48b3f8dd93"
      },
      "cell_type": "code",
      "source": [
        "numEpochs = 2\n",
        "\n",
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 2\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/2\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.4122 - acc: 0.9656 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 2/2\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.2925 - acc: 0.9806 - val_loss: 4.9167e-04 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5328cf0eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "WHbaWY43wLo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "ddedb973-8b61-40df-937b-fa465884336d"
      },
      "cell_type": "code",
      "source": [
        "numEpochs = 2\n",
        "\n",
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 2\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/2\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.2446 - acc: 0.9844 - val_loss: 4.8975e-05 - val_acc: 1.0000\n",
            "Epoch 2/2\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.2632 - acc: 0.9800 - val_loss: 0.0038 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5309071898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "KlQT28ttxJxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b353ceaa-be27-4346-d104-c2388b1cd9e6"
      },
      "cell_type": "code",
      "source": [
        "numEpochs = 1\n",
        "\n",
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 1\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/1\n",
            "1600/1600 [==============================] - 122s 76ms/step - loss: 0.3722 - acc: 0.9762 - val_loss: 9.4890e-05 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f530fff37f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "f_DmFtBzvrwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qexmaPZVz04q",
        "colab_type": "code",
        "outputId": "b5a5509e-d919-48f3-b216-9d733b1d3434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(Y_test[:, 1], 'r')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(Y_pred[:, 1], 'b')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(Y_test[:, 1] - Y_pred[:, 1], 'g')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XtcVPW+P/7XYobbwMjNGcLQchMG\noVaUe2ckmhcq6nRxp1KbYz4e2uWLuNseLfmSOzzfvOQlHxXVKdl6dr9sG6Xutp3a4int7LYSprZJ\nyC3BKQIjmAEEBgZwZtbvj4mJJcPVxcxa4+v5eBSsWcNanzcf4cX6fNZFEEVRBBERESmen7cbQERE\nREPD0CYiIlIJhjYREZFKMLSJiIhUgqFNRESkEgxtIiIildDKvcGKigpkZWVhyZIlyMzMlKw7duwY\ntm/fDo1Gg9TUVCxfvnzAbZlMbXI3DxEROjQ3d8i+XW/wlVp8pQ6AtSgVa1Em1uKewaDvd52sR9od\nHR147rnnMH36dLfr169fj/z8fOzZswdHjx5FZWWlnLsfEq1W4/F9jhZfqcVX6gBYi1KxFmViLSPY\nj5wbCwgIQEFBAQoKCvqsq6mpQVhYGGJiYgAAM2fORHFxMa655ho5m0BEcmlthdDa6u1WyCPAAaFV\n/pE7r2AtiiJq/QGdzmP7kzW0tVottFr3mzSZTIiMjHQtR0ZGoqamRs7dE5FMQtatBV57GWO93RAZ\nsRZlUnstokaDlj37gAX3eWR/ss9pyykiQjcqQw4DzReoja/U4it1AD5SyzdnnB/vuQfw4/mqRP0R\ndDqET7segGd+9j0W2kajEWaz2bVcX18Po9E44NeMxgkKBoN+VE5w8wZfqcVX6gB8p5awzm4EADAV\nvAVo1D/v6Cv9ArAWpTJAvpOnPXYi2kBiY2NhsVhQW1sLm82GI0eOICUlxVO7J6LhsNmcH3mUTaQo\nsh5pl5WVYfPmzTh37hy0Wi2Kioowe/ZsxMbGYt68eVi3bh1WrVoFAEhPT8fEiRPl3D0RyUSw252B\nLQjebgoR9SJraE+ePBlvvfVWv+unTZuGwsJCOXdJRKPBbgf6OamUiLyHY19E1JfD7hNz2US+hqFN\nRH3ZGNpESsTQJqI+BDtDm0iJGNpE1JeDc9pESsTQJqK+bDYeaRMpEEObiPri8DiRIjG0iagPwe5g\naBMpEEObiPrinDaRIjG0iagvzmkTKRJDm4j64CVfRMrE0CaivnhHNCJFYmgTUV+89ziRIjG0iagv\n3saUSJEY2kTUh8DhcSJFYmgTUV88EY1IkRjaRNQX57SJFImhTUR9CLxOm0iRGNpEJOVwOD8ytIkU\nh6FNRFJ2u/Mjh8eJFIehTURSPaHNI20ixWFoE5GUzeb8yNAmUhyGNhFJCA4eaRMpFUObiKQ4p02k\nWAxtIpKy8+xxIqViaBORhGDnnDaRUjG0iUiKZ48TKRZDm4ikOKdNpFgMbSKS4iVfRIrF0CYiCV7y\nRaRcDG0ikuLZ40SKxdAmIinOaRMpluw/lRs3bkRpaSkEQUBubi6mTp3qWjd79mxcccUV0Pz0F/y2\nbdsQHR0tdxOI6FJwTptIsWQN7ePHj6O6uhqFhYWoqqpCbm4uCgsLJe8pKChASEiInLslIhlxTptI\nuWQdHi8uLsbcuXMBAHFxcWhpaYHFYpFzF0Q02nidNpFiyRraZrMZERERruXIyEiYTCbJe/Ly8vDQ\nQw9h27ZtEEVRzt0TkRw4p02kWKP6U3lxKP/2t7/FjBkzEBYWhuXLl6OoqAh33nlnv18fEaGDViv/\nX/sGg172bXqLr9TiK3UAPlCLPtD5UaNRfy29sBZlYi3DI2toG41GmM1m13JDQwMMBoNr+f7773d9\nnpqaioqKigFDu7m5Q87mAXB+U02mNtm36w2+Uouv1AH4Ri3+jW0IBwCNRvW19PCFfunBWpRJzloG\nCn9Zh8dTUlJQVFQEACgvL4fRaERoaCgAoK2tDUuXLkV3dzcA4IsvvkB8fLycuyciOXB4nEixZP2p\nTE5ORlJSEjIyMiAIAvLy8rB//37o9XrMmzcPqampWLRoEQIDA3HdddcNeJRNRF7CE9GIFEv2P6VX\nr14tWU5ISHB9/sgjj+CRRx6Re5dEJCM+mpNIuXhHNCKS4m1MiRSLoU1EUpzTJlIshjYRSXFOm0ix\nGNpEJME5bSLlYmgTkRSPtIkUi6FNRFKc0yZSLIY2EUnxSJtIsRjaRCQhMLSJFIuhTURSDG0ixWJo\nE5EU57SJFIuhTUQSvOSLSLkY2kQkxeFxIsViaBORFO89TqRYDG0ikuKcNpFiMbSJSIJz2kTKxdAm\nIinOaRMpFkObiKQ4PE6kWAxtIpJy8EibSKkY2kQkIdgY2kRKxdAmIinOaRMpFkObiKQ4p02kWAxt\nIpLinDaRYjG0iUhCsPE6bSKlYmgTkRRvY0qkWAxtIpJycE6bSKkY2kQkweFxIuViaBORFC/5IlIs\nhjYRSTG0iRSLoU1EUpzTJlIshjYRSfA2pkTKxdAmIikOjxMpluyhvXHjRixatAgZGRn46quvJOuO\nHTuGBx98EIsWLcKrr74q966JSA68IxqRYska2sePH0d1dTUKCwuxYcMGbNiwQbJ+/fr1yM/Px549\ne3D06FFUVlbKuXsikgPvPU6kWLL+VBYXF2Pu3LkAgLi4OLS0tMBisSA0NBQ1NTUICwtDTEwMAGDm\nzJkoLi7GNddcI2cTBiRY2gBrM/waLR7b56iyhvpGLb5SB+ATtQjt7c5PNBoADq+2hYikZA1ts9mM\npKQk13JkZCRMJhNCQ0NhMpkQGRkpWVdTUyPn7gfW3o7IqQmApQ1RntvrqPOVWnylDsA3ahEFAYJW\nC+CCt5tCRL2M6viXKIqX9PURETpotTLNq40NBVb+DvjuO3m2R+TDhJtuArRaGAx6bzdFNqxFmVjL\n8Mga2kajEWaz2bXc0NAAg8Hgdl19fT2MRuOA22tu7pCzecCKp2Aw6GEytcm7XS/xlVp8pQ7Ax2oB\nfKcWX+oX1qJIctYyUPjLeiJaSkoKioqKAADl5eUwGo0IDQ0FAMTGxsJisaC2thY2mw1HjhxBSkqK\nnLsnIiLyabIeaScnJyMpKQkZGRkQBAF5eXnYv38/9Ho95s2bh3Xr1mHVqlUAgPT0dEycOFHO3RMR\nEfk0QbzUiWciIiLyCN4RjYiISCUY2kRERCrB0CYiIlIJhjYREZFKMLSJiIhUgqFNRESkEpfNY3w2\nbtyI0tJSCIKA3NxcTJ061dtNGrKSkhI8+eSTiI+PBwBMmjQJy5Ytw9NPPw273Q6DwYCtW7ciICDA\nyy0dWEVFBbKysrBkyRJkZmairq7ObQ0HDhzAm2++CT8/PyxcuBALFizwdtP7uLiWnJwclJeXIzw8\nHACwdOlSzJo1S/G1bNmyBSdPnoTNZsPjjz+OKVOmqLZPLq7l8OHDquwTq9WKnJwcNDY2oqurC1lZ\nWUhISFBlv7irpaioSJX9AgCdnZ245557kJWVhenTp3unT8TLQElJifjYY4+JoiiKlZWV4sKFC73c\nouH5/PPPxRUrVkhey8nJET/66CNRFEXxhRdeEN9++21vNG3I2tvbxczMTHHt2rXiW2+9JYqi+xra\n29vFtLQ0sbW1VbRareLdd98tNjc3e7PpfbirZc2aNeLhw4f7vE/JtRQXF4vLli0TRVEUm5qaxJkz\nZ6q2T9zVosY+EUVR/PDDD8UdO3aIoiiKtbW1Ylpammr7xV0tau0XURTF7du3i/Pnzxf37dvntT65\nLIbH+3tkqJqVlJRgzpw5AIDbb78dxcXFXm7RwAICAlBQUCC537y7GkpLSzFlyhTo9XoEBQUhOTkZ\np06d8laz3XJXiztKr2XatGl46aWXAABjxoyB1WpVbZ+4q8Xe81zwXtRQS3p6Oh599FEAQF1dHaKj\no1XbL+5qcUcNtVRVVaGyshKzZs0C4L3fX5dFaJvNZkRERLiWex4ZqiaVlZV44okn8NBDD+Ho0aOw\nWq2u4fCoqCjF16PVahEUFCR5zV0NZrO5zyNclVabu1oAYPfu3Vi8eDFWrlyJpqYmxdei0Wig0+kA\nAHv37kVqaqpq+8RdLRqNRnV90ltGRgZWr16N3Nxc1fZLj961AOr7WQGAzZs3Iycnx7XsrT65bOa0\nexNVdufWq6++GtnZ2bjrrrtQU1ODxYsXS44i1FaPO/3VoJba7rvvPoSHhyMxMRE7duzAK6+8ghtv\nvFHyHqXW8vHHH2Pv3r3YtWsX0tLSXK+rsU9611JWVqbaPgGAd955B2fOnMFTTz0laaca+6V3Lbm5\nuarrl/fffx833HADxo8f73a9J/vksjjSHuiRoWoQHR2N9PR0CIKACRMmYOzYsWhpaUFnZyeAoT3m\nVIl0Ol2fGtz1lRpqmz59OhITEwEAs2fPRkVFhSpq+eyzz/D666+joKAAer1e1X1ycS1q7ZOysjLU\n1dUBABITE2G32xESEqLKfnFXy6RJk1TXL59++ik++eQTLFy4EO+99x5ee+01r/2sXBahPdAjQ9Xg\nwIED2LlzJwDAZDKhsbER8+fPd9V06NAhzJgxw5tNHJFbb721Tw3XX389Tp8+jdbWVrS3t+PUqVO4\n+eabvdzSwa1YsQI1NTUAnHNd8fHxiq+lra0NW7ZswRtvvOE6k1etfeKuFjX2CQCcOHECu3btAuCc\n2uvo6FBtv7ir5dlnn1Vdv7z44ovYt28f3n33XSxYsABZWVle65PL5ilf27Ztw4kTJ1yPDE1ISPB2\nk4bMYrFg9erVaG1txYULF5CdnY3ExESsWbMGXV1dGDduHDZt2gR/f39vN7VfZWVl2Lx5M86dOwet\nVovo6Ghs27YNOTk5fWo4ePAgdu7cCUEQkJmZiXvvvdfbzZdwV0tmZiZ27NiB4OBg6HQ6bNq0CVFR\nUYqupbCwEPn5+ZJH5D7//PNYu3at6vrEXS3z58/H7t27VdUngPOyomeeeQZ1dXXo7OxEdnY2Jk+e\n7PbnXY216HQ6bN26VXX90iM/Px9XXnklbrvtNq/0yWUT2kRERGp3WQyPExER+QJFnz1uMrXJvs2I\nCB2amztk3643+EotvlIHwFqUirUoE2txz2DQ97tO9iPtiooKzJ07F7t37+6z7tixY3jwwQexaNEi\nvPrqq3Lveki0Wo1X9jsafKUWX6kDYC1KxVqUibUMn6yh3dHRgeeeew7Tp093u379+vXIz8/Hnj17\ncPToUVRWVsq5eyIiIp8m6/B4z+0dCwoK+qyrqalBWFgYYmJiAAAzZ85EcXExrrnmGjmbQEQyOHPG\nD6+8ArS2BsBuB/z9gTFjRHR3C+jsBC5cGN72/P2BSZMcCA4GSkv94OcHPPLIBbzzjj9aWkbWRr0e\n6O4GAgIAu93ZJj8/538ajfM/q9W53mgE6usDcOEC4HAAoij9D3C+PyJCRFSUiB9+8ENwsIiGBmFk\njRtFOh0wa5YfAgOBr77SoKMDsFoFdHQANtulb//iU5MHO1U5JAS4914bPvhAi64uIDJSRGOjIGmL\nILj/XKcDrNaAQd8n9NMNiYkO3HOPDR9/rEFJiQZXXSUiIkJEebmfq929+1gQnP2s1To/7+4e/r/l\niwUHA0uXdsNTt/6QNbS1Wi20WvebNJlMfW7v1nOtXn8iInSjMuQw0HyB2vhKLb5SB+AbtSxbBhw4\nAACBo7aP4uJAHD06apt3Y/Rq8bSXXgqBzeb8Y0UJNm++lO/tpfVLVBTQ2HhJm7hkt9wSiGuv9czP\nvqJPRBuNExQMBv2onODmDb5Si6/UAfhOLc3NwQC0+POfOxAQ4DzCbmsDAgOBoCBAqxX7Pfpxp6ND\nwNmzfmhvFxAUJOLZZ4Nw7JgIQMBzz3Xi5puHlz6iCLS2CggIcB4taTTOtjkcziCz252f96wPDNTB\nZutAQIDzCMv5n+j6HHC+//vv/dDYKOAXv3DAahUQE+OAn8KusfnxxxAsXSpCpwN+//sujB0rQqdz\nLmu1w7+CVxT7HskOttzbRx9p8dprAfg//6cbc+faYTYLGDtWRGCg6Np+7331Fh4egubm9p/WCf2+\nz93rNhvw/vtaHD6sxa9+5cBTT3Xj5EkN2tqA2bPtrqNpZ/udfe1wCHA4nF/rcDj/zQz33/LFgoKA\n665zAJDvZ3+g8PdYaF98eze13nqT6HLgcDg/Tp9uly20UlOdwdzVBeTlBbp+Sc+da0Nc3OjeLsJg\nAEymwf8w+OUvHaPaDjkYDEBMTAfCw0X84hfev83GTTd1Y/XqbgQHD/9rnf0y8u/5jBl2AF2u5Z5/\nY77MY39DxsbGwmKxoLa2FjabDUeOHEFKSoqndk9Ew2C3O49SRuMoMzAQGD/eGTYajYgJE7wfPGqT\nnOxQRGD3GElg08jIeqR98e0di4qKMHv2bMTGxmLevHlYt24dVq1aBcD5nNXetxwkIuWw2wVoRvEK\nll/8woHvv/fDhAkiFHz3XSLFkTW0J0+ejLfeeqvf9dOmTUNhYaGcuySiUWC3Y9RD+9NPnR+JaOgU\ndooFESnBaId2XJxD8pGIhoahTUR92O3Oa1lHyy232OHnJ2L6dN8/cYhIToq+5IuIvGO0j7SnTHGg\nqsqCkJDR2weRL+KRNhH1MdqhDYCBTTQCDG0i6mO0zx4nopFhaBNRH6M9p01EI8PQJqI+HI7RHx4n\nouFjaBNRHzYbQ5tIiRjaRNSHJ05EI6LhY2gTUR+c0yZSJoY2EfXBs8eJlImhTUR9cHicSJkY2kTU\nB0ObSJkY2kTUh8PBOW0iJWJoE1EfvOSLSJkY2kTUB4fHiZSJoU1EEqIIOBwCh8eJFIihTUQSDofz\nI4+0iZSHoU1EEjab8yNDm0h5GNpEJGG3Oz8ytImUh6FNRBI9w+Oc0yZSHoY2EUnwSJtIuRjaRCRh\nswkAGNpESsTQJiIJHmkTKRdDm4gkOKdNpFwMbSKS4CVfRMrF0CYiCQ6PEykXQ5uIJBjaRMrF0CYi\nCc5pEykXQ5uIJHjJF5FyMbSJSILD40TKJfsA2MaNG1FaWgpBEJCbm4upU6e61s2ePRtXXHEFND/9\nNti2bRuio6PlbgIRXQI+5YtIuWQN7ePHj6O6uhqFhYWoqqpCbm4uCgsLJe8pKChASEiInLslIhn1\nHGlzTptIeWQdHi8uLsbcuXMBAHFxcWhpaYHFYpFzF0Q0ynidNpFyyfq3tNlsRlJSkms5MjISJpMJ\noaGhrtfy8vJw7tw53HTTTVi1ahUEQeh3exEROmi18v/mMBj0sm/TW3ylFl+pA1B/LWPGOD9qNOqv\npTfWokysZXhGdQBMFEXJ8m9/+1vMmDEDYWFhWL58OYqKinDnnXf2+/XNzR2yt8lg0MNkapN9u97g\nK7X4Sh2Ab9TS2KgBoINWC9XX0sMX+qUHa1EmOWsZKPxlHR43Go0wm82u5YaGBhgMBtfy/fffj6io\nKGi1WqSmpqKiokLO3RORDHj2OJFyyRraKSkpKCoqAgCUl5fDaDS6hsbb2tqwdOlSdHd3AwC++OIL\nxMfHy7l7IpIB57SJlEvW4fHk5GQkJSUhIyMDgiAgLy8P+/fvh16vx7x585CamopFixYhMDAQ1113\n3YBD40TkHbzki0i5ZJ/TXr16tWQ5ISHB9fkjjzyCRx55RO5dEpGMeMkXkXLxjmhEJME5bSLlYmgT\nkQTvPU6kXAxtIpLgnDaRcjG0iUiCc9pEysXQJiIJXvJFpFwMbSKS4IloRMrF0CYiCc5pEykXQ5uI\nJOx259njnNMmUh6GNhFJcE6bSLkY2kQkweFxIuViaBORBC/5IlIuhjYRSfDscSLlYmgTkQRvY0qk\nXAxtIpLgnDaRcjG0iUiCc9pEysXQJiIJzmkTKRdDm4gkGNpEysXQJiIJhjaRcjG0iUiCc9pEysXQ\nJiIJXvJFpFwMbSKS4CVfRMrF0CYiCc5pEykXQ5uIJDinTaRcDG0ikuCRNpFyMbSJSIKhTaRcDG0i\nkrDbefY4kVIxtIlIgnPaRMrF0CYiCQ6PEykXQ5uIJBjaRMrF0CYiCQ6PEykXQ5uIJHikTaRcsof2\nxo0bsWjRImRkZOCrr76SrDt27BgefPBBLFq0CK+++qrcuyYiGfDscSLlkjW0jx8/jurqahQWFmLD\nhg3YsGGDZP369euRn5+PPXv24OjRo6isrJRz90QkAx5pEymXrLNWxcXFmDt3LgAgLi4OLS0tsFgs\nCA0NRU1NDcLCwhATEwMAmDlzJoqLi3HNNdfI2YR+iSKQnx+AhgbAag30yD5HW3Cwb9QSFAR0dnq3\nDlGUZztKqOVSnTnj/Fuec9pEyiPrj6XZbEZSUpJrOTIyEiaTCaGhoTCZTIiMjJSsq6mpGXB7ERE6\naLXy/Lnf3g688AJgtQJAgCzbVAZfqcVX6gB8oZYrr3SGtsGg93ZTZMNalIm1DM+o/i0tXuLhS3Nz\nh0wtcfrHPwBR1KOpySLrdr0lMjL0kmsRBJkacwnkqEMOcnwvlFLLpTIaRfj56WEytXm7KbIwGFiL\nErGW/rfVH1lD22g0wmw2u5YbGhpgMBjcrquvr4fRaJRz94OKiAAMBsBkkmks1Mt8pRZfqQPwrVqI\nSHlkPREtJSUFRUVFAIDy8nIYjUaEhoYCAGJjY2GxWFBbWwubzYYjR44gJSVFzt0TERH5NEG81DHs\ni2zbtg0nTpyAIAjIy8vD119/Db1ej3nz5uGLL77Atm3bAABpaWlYunSpnLsmIiLyabKHNhEREY0O\n3hGNiIhIJRjaREREKsHQJiIiUgmGNhERkUowtImIiFSCoU1ERKQSl80jATZu3IjS0lIIgoDc3FxM\nnTrV200aspKSEjz55JOIj48HAEyaNAnLli3D008/DbvdDoPBgK1btyIgQNn3vK6oqEBWVhaWLFmC\nzMxM1NXVua3hwIEDePPNN+Hn54eFCxdiwYIF3m56HxfXkpOTg/LycoSHhwMAli5dilmzZim+li1b\ntuDkyZOw2Wx4/PHHMWXKFNX2ycW1HD58WJV9YrVakZOTg8bGRnR1dSErKwsJCQmq7Bd3tRQVFamy\nXwCgs7MT99xzD7KysjB9+nTv9Il4GSgpKREfe+wxURRFsbKyUly4cKGXWzQ8n3/+ubhixQrJazk5\nOeJHH30kiqIovvDCC+Lbb7/tjaYNWXt7u5iZmSmuXbtWfOutt0RRdF9De3u7mJaWJra2topWq1W8\n++67xebmZm82vQ93taxZs0Y8fPhwn/cpuZbi4mJx2bJloiiKYlNTkzhz5kzV9om7WtTYJ6Ioih9+\n+KG4Y8cOURRFsba2VkxLS1Ntv7irRa39IoqiuH37dnH+/Pnivn37vNYnl8XweH+PDFWzkpISzJkz\nBwBw++23o7i42MstGlhAQAAKCgok95t3V0NpaSmmTJkCvV6PoKAgJCcn49SpU95qtlvuanFH6bVM\nmzYNL730EgBgzJgxsFqtqu0Td7XYex4M3osaaklPT8ejjz4KAKirq0N0dLRq+8VdLe6ooZaqqipU\nVlZi1qxZALz3++uyCG2z2YyIiAjXcs8jQ9WksrISTzzxBB566CEcPXoUVqvVNRweFRWl+Hq0Wi2C\ngoIkr7mrwWw293mEq9Jqc1cLAOzevRuLFy/GypUr0dTUpPhaNBoNdDodAGDv3r1ITU1VbZ+4q0Wj\n0aiuT3rLyMjA6tWrkZubq9p+6dG7FkB9PysAsHnzZuTk5LiWvdUnl82cdm+iyu7cevXVVyM7Oxt3\n3XUXampqsHjxYslRhNrqcae/GtRS23333Yfw8HAkJiZix44deOWVV3DjjTdK3qPUWj7++GPs3bsX\nu3btQlpamut1NfZJ71rKyspU2ycA8M477+DMmTN46qmnJO1UY7/0riU3N1d1/fL+++/jhhtuwPjx\n492u92SfXBZH2gM9MlQNoqOjkZ6eDkEQMGHCBIwdOxYtLS3o7OwE4J3HnMpBp9P1qcFdX6mhtunT\npyMxMREAMHv2bFRUVKiils8++wyvv/46CgoKoNfrVd0nF9ei1j4pKytDXV0dACAxMRF2ux0hISGq\n7Bd3tUyaNEl1/fLpp5/ik08+wcKFC/Hee+/htdde89rPymUR2gM9MlQNDhw4gJ07dwIATCYTGhsb\nMX/+fFdNhw4dwowZM7zZxBG59dZb+9Rw/fXX4/Tp02htbUV7eztOnTqFm2++2cstHdyKFStQU1MD\nwDnXFR8fr/ha2trasGXLFrzxxhuuM3nV2ifualFjnwDAiRMnsGvXLgDOqb2Ojg7V9ou7Wp599lnV\n9cuLL76Iffv24d1338WCBQuQlZXltT65bJ7ydfEjQxMSErzdpCGzWCxYvXo1WltbceHCBWRnZyMx\nMRFr1qxBV1cXxo0bh02bNsHf39/bTe1XWVkZNm/ejHPnzkGr1SI6Ohrbtm1DTk5OnxoOHjyInTt3\nQhAEZGZm4t577/V28yXc1ZKZmYkdO3YgODgYOp0OmzZtQlRUlKJrKSwsRH5+PiZOnOh67fnnn8fa\ntWtV1yfuapk/fz52796tqj4BnJcVPfPMM6irq0NnZyeys7MxefJktz/vaqxFp9Nh69atquuXHvn5\n+bjyyitx2223eaVPLpvQJiIiUrvLYniciIjIFyj67HGTqU32bUZE6NDc3CH7dr3BV2rxlToA1qJU\nrEWZWIt7BoO+33WX3ZG2VqvxdhNk4yu1+EodAGtRKtaiTKxl+C670CYiZeDpNETDx9AmIo/7prkC\nSX+8Bp/WHPZ2U4hUhaFNRB5X0XwWZqsJ5eYybzeFSFUY2kTkcQ7ReRteu9j3oR5E1D+GNhF5nM1h\nAwDYf/pIREPD0CYij7PzSJtoRBjaRORxdgdDm2gkGNpE5HGuI20HQ5toOBjaRORxHB4nGpkh3cZ0\n48aNKC0thSAIyM3NxdSpU13rjh07hu3bt0Oj0SA1NRXLly/He++9hwMHDrjeU1ZWhi+//BL/+q//\nio6ODuh0OgDAmjVrMHnyZJlLIiKl4/A40cgMGtrHjx9HdXU1CgsLUVVVhdzcXBQWFrrWr1+/Hjt3\n7nQ9nvCOO+7AggULsGDBAtfX//Wvf3W9f9OmTZg0adIolEJEamEXfzp7nKFNNCyDDo8XFxdj7ty5\nAIC4uDi0tLTAYrEAAGpqahB+uSilAAAcIElEQVQWFoaYmBj4+flh5syZKC4ulnz9q6++iqysrFFo\nOhGpletIm5d8EQ3LoEfaZrMZSUlJruXIyEiYTCaEhobCZDIhMjJSsq6mpsa1/NVXXyEmJgYGg8H1\n2ssvv4zm5mbExcUhNzcXQUFB/e47IkI3KjdhH+gJKmrjK7X4Sh0AaxmK4BB/AEBAoMZj3y/2izKx\nluEZ9qM5h3OT/7179+KBBx5wLS9evBjXXnstJkyYgLy8PLz99ttYunRpv18/Go9sMxj0o/LIT2/w\nlVp8pQ6AtQxVS5vzZ9vS0emR7xf7RZlYS//b6s+gw+NGoxFms9m13NDQ4DpyvnhdfX09jEaja7mk\npAQ33nija3nevHmYMGECAGD27NmoqKgYRhlE5Ct6hsV75raJaGgGDe2UlBQUFRUBAMrLy2E0GhEa\nGgoAiI2NhcViQW1tLWw2G44cOYKUlBQAzgAPCQlBQEAAAOcR+pIlS9Da2grAGejx8fGjUhQRKRsv\n+SIamUGHx5OTk5GUlISMjAwIgoC8vDzs378fer0e8+bNw7p167Bq1SoAQHp6OiZOnAgAfea7BUHA\nwoULsWTJEgQHByM6OhorVqwYpbKISMl4cxWikRnSnPbq1aslywkJCa7Pp02bJrkErMfkyZPxhz/8\nQfJaeno60tPTR9JOIvIhHB4nGhneEY2IPM4uOpwfHQ4vt4RIXRjaRORxnNMmGhmGNhF5nOt52gxt\nomFhaBORxzlE3hGNaCQY2kTkcRweJxoZhjYReZyNT/kiGhGGNhF5nIPXaRONCEObiDyOz9MmGhmG\nNhF5nI3P0yYaEYY2EXlcz5G2jWePEw0LQ5uIPK5nTtvBI22iYWFoE5HH2VyXfPE2pkTDwdAmIo/j\n8DjRyDC0icjjODxONDJDejTnxo0bUVpaCkEQkJubi6lTp7rWHTt2DNu3b4dGo0FqaiqWL1+OkpIS\nPPnkk4iPjwcATJo0Cb///e9RV1eHp59+Gna7HQaDAVu3bkVAQMDoVEZEiuW69ziv0yYalkFD+/jx\n46iurkZhYSGqqqqQm5sreX72+vXrsXPnTkRHRyMzMxN33HEHAOCXv/wlXn75Zcm2Xn75ZTz88MO4\n6667sH37duzduxcPP/ywzCURkdLxNqZEIzPo8HhxcTHmzp0LAIiLi0NLSwssFgsAoKamBmFhYYiJ\niYGfnx9mzpyJ4uLifrdVUlKCOXPmAABuv/32Ad9LRL6r5wQ0G0ObaFgGPdI2m81ISkpyLUdGRsJk\nMiE0NBQmkwmRkZGSdTU1NZg0aRIqKyvxxBNPoKWlBdnZ2UhJSYHVanUNh0dFRcFkMg2474gIHbRa\nzUhr65fBoJd9m97iK7X4Sh0AaxkKTc9vHsHhse8X+0WZWMvwDGlOuzdRFAd9z9VXX43s7Gzcdddd\nqKmpweLFi3Ho0KFhb6e5uWO4zRuUwaCHydQm+3a9wVdq8ZU6ANYyVNauLgDABZvNI98v9osysZb+\nt9WfQYfHjUYjzGaza7mhoQEGg8Htuvr6ehiNRkRHRyM9PR2CIGDChAkYO3Ys6uvrodPp0NnZKXkv\nEV1+euayeckX0fAMGtopKSkoKioCAJSXl8NoNCI0NBQAEBsbC4vFgtraWthsNhw5cgQpKSk4cOAA\ndu7cCQAwmUxobGxEdHQ0br31Vte2Dh06hBkzZoxWXUSkYLzki2hkBh0eT05ORlJSEjIyMiAIAvLy\n8rB//37o9XrMmzcP69atw6pVqwAA6enpmDhxIgwGA1avXo1PPvkEFy5cwLp16xAQEIAVK1ZgzZo1\nKCwsxLhx43D//fePeoFEpDx8njbRyAxpTnv16tWS5YSEBNfn06ZNk1wCBgChoaF4/fXX+2zHaDTi\nP//zP0fSTiLyIT8PjzO0iYaDd0QjIo/ruakKh8eJhoehTUQeZ+fztIlGhKFNRB7HO6IRjQxDm4g8\njk/5IhoZhjYReVzvI2wHn6lNNGQMbSLyuN5P9+KTvoiGjqFNRB7X+0jbJnKInGioGNpE5HH2XkHN\nk9GIho6hTUQe13tI3MHhcaIhY2gTkcfZe518xuFxoqFjaBORx/W+1Mvu4NnjREPF0CYij3NILvni\n8DjRUDG0icjjJGeP8wYrREM2pKd8bdy4EaWlpRAEAbm5uZg6dapr3bFjx7B9+3ZoNBqkpqZi+fLl\nAIAtW7bg5MmTsNlsePzxx5GWloacnByUl5cjPDwcALB06VLMmjVL/qqISNF6hzbPHicaukFD+/jx\n46iurkZhYSGqqqqQm5sreRTn+vXrsXPnTkRHRyMzMxN33HEHzGYzvvnmGxQWFqK5uRkPPPAA0tLS\nAAD/9m//httvv330KiIiRRNFUXIXNIY20dANGtrFxcWYO3cuACAuLg4tLS2wWCwIDQ1FTU0NwsLC\nEBMTAwCYOXMmiouL8fDDD7uOxseMGQOr1Qq7nT+YRNQ3pDmnTTR0g85pm81mREREuJYjIyNhMpkA\nACaTCZGRkX3WaTQa6HQ6AMDevXuRmpoKjUYDANi9ezcWL16MlStXoqmpSdZiiEj5Lg5tG6/TJhqy\nIc1p9yaK4pDf+/HHH2Pv3r3YtWsXAOC+++5DeHg4EhMTsWPHDrzyyit49tln+/36iAgdtFrNcJs4\nKINBL/s2vcVXavGVOgDWMpj2bumxQlh4kEe+Z+wXZWItwzNoaBuNRpjNZtdyQ0MDDAaD23X19fUw\nGo0AgM8++wyvv/46/vCHP0CvdxYyffp013tnz56NdevWDbjv5uaOoVcyRAaDHiZTm+zb9QZfqcVX\n6gBYy1C0dbdKlk1NrTD5je73jP2iTKyl/231Z9Dh8ZSUFBQVFQEAysvLYTQaERoaCgCIjY2FxWJB\nbW0tbDYbjhw5gpSUFLS1tWHLli144403XGeKA8CKFStQU1MDACgpKUF8fPwlFUZE6nPxU73svOSL\naMgGPdJOTk5GUlISMjIyIAgC8vLysH//fuj1esybNw/r1q3DqlWrAADp6emYOHGi66zx3/3ud67t\nbN68Gb/5zW/wu9/9DsHBwdDpdNi0adPoVUZEimS7aE6bZ48TDZ0gDmeS2sNGY9iEwzHK4yt1AKxl\nKOo76jHljz+Psv3XA/+NX8b8Svb99MZ+USbW0v+2+sM7ohGRR138VC87HxhCNGQMbSLyqIuf6sXh\ncaKhY2gTkUf1PRGNoU00VAxtIvKoi++AxiPt4dv6xSa8e3aPt5tBXsDQJiKPuvgOaLzka3hsDhu2\nfrEJr/0j39tNIS9gaBORR118ZG3v9fAQGlyT1Xn7Z7PV5OWWkDcwtInIo/qGNofHh6Oxo9H50WqW\nPC3NWzptnfhj2U502bu83ZTLAkObiDyqZzjc389fskxDY+5w3jraLtrR1On9hy69X7kPT/9tJf78\nzV5vN+WywNAmIo/qObL29wuQLNPQNFobXZ8rYYi8pu17AEBtW41X9r/ySDZWfPKEV/btDQxtIvKo\nnpAO1DC0R6JneBwATB0NXmyJ04/tP0o+elKZ+TTePvP/ofDsnxQxVeAJDG0i8qie67IDNIEAnGdD\n09D1DI8DgMnq/dBu6PhR8tGTCr76D9fnJgWMOnjCsJ+nTUR0KXqOrAN+OtK+XI6Q5NJ7eFxZR9p1\nHt2vKIoo+u4j1/K5thr87/lKmK1m/EvcfR5tiycxtInIoy4ObQ6PD0/v4XGz1TzAOz2j/qcj7PqO\neo/ut8HaIDkR75ylFhtL/h+qW7/D3KvOIVgb7NH2eAqHx4nII75vrcYLJzbjfGczACDAT3nD40fP\nfYYXTmyGgh9+qKgjbbvDjoafwrqho96jt6Q923QGAHBtRAIA4Ezj16g6Xwmbw4avG8s81g5PG9KR\n9saNG1FaWgpBEJCbm4upU6e61h07dgzbt2+HRqNBamoqli9f3u/X1NXV4emnn4bdbofBYMDWrVsR\nEBAwOpURkWKcafwaMwtvAQDcYLgRwM8nol18W1Nveubva/B1YxlSY2dhytjrEaQN8naT+lDSnLa5\n8+drxe2iHY2djTDqjB7Z9z8bvwYAzLkqDWeb/4lD1Qdd6/7R8CVuip7mkXZ42qBH2sePH0d1dTUK\nCwuxYcMGbNiwQbJ+/fr1yM/Px549e3D06FFUVlb2+zUvv/wyHn74YfzpT3/CVVddhb17eV0fkRr8\n7/lKvHf2nSEdSbV2tfR57aVTL7g+/4fpSwCAf8/w+AiPzj6vK0ZtWw0OffdXnDZ/5XpdFEX8xz9e\nwUsnX0Bbdyve+vqP2F68fdC586rz37iO0O7ePw9T3pzkupzpYqN5JG61WV3bF0URZ5v+CUv3z89p\nbrQ2IiooCqH+ehz74Sg+qHp/yO3Z9sXzWPjB/WiQ6Qi94aIzxv/rf/+C2/ZMw8fVRc71HQ2j9r06\n2/xPAMCcCfMAAF+Z/uFa94+GU7Lu64L9Ar6sP6mI8y8GPdIuLi7G3LlzAQBxcXFoaWmBxWJBaGgo\nampqEBYWhpiYGADAzJkzUVxcjKamJrdfU1JSgn//938HANx+++3YtWsXHn744dGqrY/Shi9haWxC\nS4tV8rogCNJlSJfdv0dqUkQCwgLD8HldsceG1sIag2Fp68avrrgFZY2n0drV6pH9jlRYYBiuN9yA\nYz/8XXL/6bDG4D59MlIX99OwvtZNvw+Fzl+HscEG1Lf/iNDzAWg+b4FddEAURQiCgEBNAKKCxuLH\njh9lGz7U+Glw25WpCPEPQZn5NL5vrZZluwAQrA3GqYYT+GdLGcbrJiIpajJ+f/T/wmw14Y/lO5Ea\nOwtXjbkawdpgmK1mVJ3/BvER12JMwBgc/PYj/KVqPx6d8gR+FTMd3Y5uWLot+KDqfVwbkYBY/Xh8\n8v1/AwACfzp7fP8376HbcQFGnRHNnU3467cfYvLYKbjReBNaulugFbTQ+etwrq0WYwLDEBkUicrz\n32Dz8Q0I9dfDcqENQZog3Bh9EzoudCBYG4zP644BAHZ89R+uo9E/Rb+DlHEzEKsfDxEiHKIDouhw\nfoSId/75J8n3oaXrPB48cC+yb/wd/P38YeluQ3TIFfi4+hAOfvshlt/4O1wZeiXOd53Hj+11uCY8\nHgGaADh+2mZzZxMO13yMc221uGviPZgUeS38/fzhJ/Q+VhJ++r/zY03b93j++HO4LioJv45fiF1l\nBag8/w3iwq9B7q+ehUbQ4kfLjxgbZMBT0/4vnjychaVFi6HThiAhMgF3TbwHIf4huCJkHESIEEUH\nKprPov1COyKDo7Dli40AgPT9c7EyeTUAwGrrgFEXjW5HN7rt3XCIDgRrgyEIAlq6WhCoCYTVZoUh\n2ID2C+04Z6lFoDYIof6hqG79DgAQFhiOlq7zyPnbKgDAyiMrcH/8r/FG6au4c+LdWHTtz7/nO21W\nFP9wDL8Ij8P1469DS4sVDtEBk7UBIf4hCPEP/fmPFogAfv4Dpuc1h+jAsR/+Dq2fFr+MuQXB2mBY\nbT//Dik8+yfcPmEOWrpaYLlgwfjQ8Wi/0I6jP3yGOssPGBMYhq9M/8CTyaswJnAM2rrbEB4YDo2g\nhSAIECDAbDXhbNMZTDFcjz9/sxdHaj7Br+MX4l/i7gcA1LZ9j6Lqg5gZOwtZN/x2JD9qIzJoaJvN\nZiQlJbmWIyMjYTKZEBoaCpPJhMjISMm6mpoaNDc3u/0aq9XqGg6PioqCyeS5U/QtFyy4Y9/to/aX\nUoBfAKKCx6Ku/YdR2f5A/AQ/RfwFOBSBmkDe7lAm1xtuxPz4Bcg7luuR/U0eOxVf/FiCL34sGfB9\ngZpAFJx+HQWnX5e8vmzqE9AIGldoXxHi/GP/VMNJnGo4KXnvsR/+Pmh7QvxDYbnQhrDAcHTarPj8\nh2MI0ASgy96FayMSMO/qO/Hqly8hWBuMW2JvwZHvjuBk/RcDbjPUX4/brpyBg999hLjwa1B1vhKr\nPu37C1mAgPWf5w3aRgAI0gThxVPbhvTenvefrD+Bk/UnoBE0mD4uBcU/HMXSosWu90yOmooH4h/E\n5LFT8dznefj2fJXb7+PFNIIGDyVk4k//fAsrP80ecpsGkzJuBj769gMAcH3f3ih9FQF+ATj47Yc4\n+O2Hsu2rt+njUhCoCcTVYybiTNPX0AeMwU3RN+PTmsN44r+XDvr1T/9t5ZD3FeIfin3fvIt937wr\nef2z2k8xQX8VHo1eMtzmj8iwzx4fyVGku68ZynYiInTQajXD3p87Buixb+E+fHf+uwHbIUIccL27\n97R3t2Pj3zeirv0HPJr8KK4zXCdLm4fC3GHGX87+BdPGTcPU6KmDf4EXFdcW42DlQWT/MhuxY2Jl\n3/6ljHBc3KfD0WxtRkN7A8aHjYe/nz80fhr4CX7wE/wgiiIs3RbX+p4zpi/V/1T/D97/5/soNX0J\ng86ANSlroPGT52eltasVYYFhePC6B3G64TTOms/i2rHX4s5r7kRtay3OmM6guqUaXbYuBGmDcJ3h\nOlQ1V6HjQgd0/jrcHX83DlYehLnDjEBtIHT+OkQGR+KOuDvgEB2IDNNDhIhfJ/4aD1TciyZrE8bp\nx6GhvQFB2iDMnjgbpT+Wora1FuFB4RAhorWrFeP049DS2YK27jbYHXbcPelufNv8LSaETYDGT4MA\nTQDG6cfB1G7CWN1YaPw0yEzOgNZPi5vG3YRmazO+/PFL1FvqXf3jJ/hBEAT4CX4I8Q/B1Oip0Afq\n0djRCD/BDwcrDyJAE4BOWyf0gXo0tDfAoDMgZUIKPqz4EH6CH0IDQnFF6BWobKqECNG5TQgI1AZi\nzsQ5CA8KxwcVH6C1qxUX7Bdc/9Z6/3vtec1P8MO/TPoXNLQ34Kv6r3Dr+FuRaEjER998hIrGCtf7\n77zmThjG6mEw3IS/TvovAEBZQxm+bf4WrV2tMHWYoBGc/w5j9DEYEzgG1eercfO4m3H9Fdcjp+Ep\n/K36bwjSBiHYPxgN7Q0I1AQiUBsIP8EPHRc64BAdiAiKQKetE0HaIJg7zAj2D8aEsAm4YL+Atu42\nWLot0AgaLEhagA/OfoCI4AjM/cVc/PnMn9FkbcL9CffjUNUh10NOAOdoWHJMMr5p/AYtvaZSDDoD\nLN0WdNo6Xe8DnH8g9f68Z93V4VdjzsQ5CNQGYs+CP+HId0dw87ibkRyTjP/57n/w3fnvEOwfDH2A\nHnWWOugD9Lgq/CpEh0TjbONZJBmS8NfKv0Lrp8WYwDE433neNUoCOEecEsYmoNxUjqjgKKTFpWHv\n13vRfqEdgPOP0zm/mIPT9adx96S7nTUY9LL8DA5EEAf5TZefnw+DwYCMjAwAwJw5c/CXv/wFoaGh\nqK2txapVq1BYWAgAeOWVVxAeHo7m5ma3X3Pffffhww8/RFBQEI4fP47du3fj5Zdf7nffJlNbv+tG\nymDQj8p2j537O/63pQq/SVx8SUO0wzFatYymniHj3tRYR388VUuXvQvrP1+HYE0wfnPdYlw15mrZ\n98F+USbWokxy1jJQ+A96IlpKSgqKipwnFZSXl8NoNCI0NBQAEBsbC4vFgtraWthsNhw5cgQpKSn9\nfs2tt97qev3QoUOYMWPGJRenFLdeeRsyr3vEY4GtVvz+yCNQE4jnUjYh95ZnRyWwiUiZBh0eT05O\nRlJSEjIyMiAIAvLy8rB//37o9XrMmzcP69atw6pVzpMP0tPTMXHiREycOLHP1wDAihUrsGbNGhQW\nFmLcuHG4//77R7c6IiIiHzLo8Lg3qWl43Bt8pRZfqQNgLUrFWpSJtfS/rf4oOrSJiIjoZ7yNKRER\nkUowtImIiFSCoU1ERKQSDG0iIiKVYGgTERGpBEObiIhIJYZ973G1GuiZ4EpXUlKCJ598EvHx8QCA\nSZMmYdmyZap7NnlFRQWysrKwZMkSZGZm9vt89QMHDuDNN9+En58fFi5ciAULFni76X1cXEtOTg7K\ny8sRHh4OAFi6dClmzZql+Fq2bNmCkydPwmaz4fHHH8eUKVNU2ycX13L48GFV9onVakVOTg4aGxvR\n1dWFrKwsJCQkqLJf3NVSVFSkyn4BgM7OTtxzzz3IysrC9OnTvdMn4mWgpKREfOyxx0RRFMXKykpx\n4cKFXm7R8Hz++efiihUrJK/l5OSIH330kSiKovjCCy+Ib7/9tjeaNmTt7e1iZmamuHbtWvGtt94S\nRdF9De3t7WJaWprY2toqWq1W8e677xabm5u92fQ+3NWyZs0a8fDhw33ep+RaiouLxWXLlomiKIpN\nTU3izJkzVdsn7mpRY5+Ioih++OGH4o4dO0RRFMXa2loxLS1Ntf3irha19osoiuL27dvF+fPni/v2\n7fNan1wWw+P9PRNczUpKSjBnzhwAzmeTFxcXe7lFAwsICEBBQQGMRqPrNXc1lJaWYsqUKdDr9QgK\nCkJycjJOnZL3gfaXyl0t7ii9lmnTpuGll14CAIwZMwZWq1W1feKuFru977PL1VBLeno6Hn30UQBA\nXV0doqOjVdsv7mpxRw21VFVVobKyErNmzQLgvd9fl0Vom81mREREuJZ7nu+tJpWVlXjiiSfw0EMP\n4ejRo159NvlIaLVaBAUFSV5zV4PZbO7zjHal1eauFgDYvXs3Fi9ejJUrV6KpqUnxtWg0Guh0OgDA\n3r17kZqaqto+cVeLRqNRXZ/0lpGRgdWrVyM3N1e1/dKjdy2A+n5WAGDz5s3IyclxLXurTy6bOe3e\nRJXdufXqq69GdnY27rrrLtTU1GDx4sWSowi11eNOfzWopbb77rsP4eHhSExMxI4dO/DKK6/gxhtv\nlLxHqbV8/PHH2Lt3L3bt2oW0tDTX62rsk961lJWVqbZPAOCdd97BmTNn8NRTT0mfva3CfuldS25u\nrur65f3338cNN9yA8ePHu13vyT65LI60jUYjzGaza7mhoQEGg8GLLRqe6OhopKenQxAETJgwAWPH\njkVLSws6O50Pi6+vrx90qFaJdDpdnxrc9ZUaaps+fToSExMBALNnz0ZFRYUqavnss8/w+uuvo6Cg\nAHq9XtV9cnEtau2TsrIy1NXVAQASExNht9sREhKiyn5xV8ukSZNU1y+ffvopPvnkEyxcuBDvvfce\nXnvtNa/9rFwWoT3QM8HV4MCBA9i5cycAwGQyobGxEfPnz1f9s8ndPV/9+uuvx+nTp9Ha2or29nac\nOnUKN998s5dbOrgVK1agpqYGgHOuKz4+XvG1tLW1YcuWLXjjjTdcZ/KqtU/c1aLGPgGAEydOYNeu\nXQCcU3sdHR2q7Rd3tTz77LOq65cXX3wR+/btw7vvvosFCxYgKyvLa31y2Tzla9u2bThx4oTr+d4J\nCQnebtKQWSwWrF69Gq2trbhw4QKys7ORmJiINWvWoKurC+PGjcOmTZvg7+/v7ab2q6ysDJs3b8a5\nc+eg1WoRHR2Nbdu2IScnp08NBw8exM6dOyEIAjIzM3Hvvfd6u/kS7mrJzMzEjh07EBwcDJ1Oh02b\nNiEqKkrRtRQWFiI/Px8TJ050vfb8889j7dq1qusTd7XMnz8fu3fvVlWfAM7Lip555hnU1dWhs7MT\n2dnZmDx5stufdzXWotPpsHXrVtX1S4/8/HxceeWVuO2227zSJ5dNaBMREandZTE8TkRE5AsY2kRE\nRCrB0CYiIlIJhjYREZFKMLSJiIhUgqFNRESkEgxtIiIilWBoExERqcT/DyjZgAlhmsHNAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f53050622e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IPTFOJh3DTgq",
        "colab_type": "code",
        "outputId": "5470e038-8cf6-4dfe-bd3d-37e6cd1b2214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "\n",
        "# modelSaved = '{}-{}_{}_{}.h5'.format(folderNormal, folderFault, pretrainedModel, now.strftime('%m-%d-%H:%M:%S'))\n",
        "\n",
        "modelSaved = '{}-{}_{}_{}.h5'.format('A1', 'A5', pretrainedModel, now.strftime('%m-%d-%H:%M:%S'))\n",
        "meanSaved = 'mean_{}.npy'.format(now.strftime('%m-%d-%H:%M:%S'))\n",
        "stdSaved = 'std_{}.npy'.format(now.strftime('%m-%d-%H:%M:%S'))\n",
        "\n",
        "inputStr = input('''Save Model as '{}'? (y/n)\\n'''.format(modelSaved))\n",
        "\n",
        "if (inputStr == 'y' or inputStr == 'Y'):  \n",
        "    model.save('gdrive/My Drive/Colab/Model/{}'.format(modelSaved))\n",
        "    np.save('gdrive/My Drive/Colab/Model/{}'.format(meanSaved), trainMean)\n",
        "    np.save('gdrive/My Drive/Colab/Model/{}'.format(stdSaved), trainStd)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Model as 'A1-A5_ResNet50_11-16-13:39:39.h5'? (y/n)\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}