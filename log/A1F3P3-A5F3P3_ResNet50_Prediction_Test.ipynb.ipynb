{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning Clean Up.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Joovvhan/Master-Thesis/blob/master/log/A1F3P3-A5F3P3_ResNet50_Prediction_Test.ipynb.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "c3821KWLNey6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ececf9f4-fa4b-48f3-aaa9-cd931e915cda"
      },
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wf\n",
        "import time\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "# Import Keras modules\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nxSASpgF7unA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.listdir('gdrive/My Drive/Colab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rw9NU8Xjwcz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set data directories\n",
        "\n",
        "# dataPath = 'gdrive/My Drive/Colab/Data'\n",
        "dataPath = '''D:\\\\0_Joowhan's Paper\\\\Synthesized\\\\Total'''\n",
        "modelPath = '''D:/0_Joowhan's Paper/Synthesized/Total/Model'''\n",
        "\n",
        "# Changed variable names to normal and fault\n",
        "# Changed variable names from folder to path\n",
        "# Need to consider multiple folders\n",
        "# Need to add files to be tested\n",
        "\n",
        "# folderNormal = 'A3F1P3'\n",
        "# folderFault = 'A3F5P3'\n",
        "\n",
        "folderNormal = 'A1F3P3'\n",
        "folderFault = 'A5F3P3'\n",
        "\n",
        "\n",
        "pathNormal = dataPath + '/' + folderNormal\n",
        "pathFault = dataPath + '/' + folderFault\n",
        "\n",
        "filesNormal = os.listdir(pathNormal)\n",
        "filesNormal = [file for file in filesNormal if file.endswith('.wav')]\n",
        "\n",
        "filesFault = os.listdir(pathFault)\n",
        "filesFault = [file for file in filesFault if file.endswith('.wav')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgGP7PZO-rTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specgram settings\n",
        "\n",
        "nsc = 1470\n",
        "nov = nsc/2\n",
        "nff = nsc \n",
        "imgSize = 224\n",
        "\n",
        "pretrainedModel = 'VGG19'\n",
        "pretrainedModel = 'Xception'\n",
        "pretrainedModel = 'ResNet50'\n",
        "lastActivation = 'softmax'\n",
        "sizeBatch = 4\n",
        "numEpochs = 8\n",
        "verb = 1\n",
        "\n",
        "\n",
        "# Learning parameters\n",
        "\n",
        "trainingRatio = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AskF2qFT2xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43869f28-15f3-44c5-8977-b8b19e8ec11d"
      },
      "cell_type": "code",
      "source": [
        "# Execution confirmed with new variable names\n",
        "\n",
        "# Check whether npy file exists \n",
        "\n",
        "npyNormalPath = glob.glob(pathNormal + '/' + '*.npy')\n",
        "\n",
        "if (len(npyNormalPath) == 1):\n",
        "    imgsNormal = np.load(npyNormalPath[0])\n",
        "\n",
        "else:\n",
        "    imgsNormal = np.zeros([len(filesNormal), imgSize, imgSize])\n",
        "    \n",
        "    for i in trange(len(filesNormal)):\n",
        "        fs, dataInt16 = wf.read(pathNormal + '/' + filesNormal[i])\n",
        "        dataFloat = dataInt16 / (2 ** 15)\n",
        "        Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "                                           window=np.hamming(nsc), cmap='viridis')\n",
        "        plt.close()\n",
        "        imgsNormal[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "    np.save(pathNormal + '/' + folderNormal + '.npy', imgsNormal)\n",
        "        \n",
        "print('Normal Image Shape: {}'.format(imgsNormal.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_MZtpk3Br_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d788b046-317d-4405-c653-cbf9fe356154"
      },
      "cell_type": "code",
      "source": [
        "# Execution confined with new varible names\n",
        "\n",
        "# Check whether npy file exists\n",
        "\n",
        "npyFaultPath = glob.glob(pathFault + '/' + '*.npy')\n",
        "\n",
        "if (len(npyFaultPath) == 1):\n",
        "    imgsFault = np.load(npyFaultPath[0])\n",
        "\n",
        "else:\n",
        "    imgsFault = np.zeros([len(filesFault), imgSize, imgSize])\n",
        "\n",
        "    for i in trange(len(filesFault)):\n",
        "        fs, dataInt16 = wf.read(pathFault + '/' + filesFault[i])\n",
        "        dataFloat = dataInt16 / (2 ** 15)\n",
        "        Pxx, _, _, _ = plt.specgram(dataFloat, NFFT=nff, Fs=fs, noverlap=nov, \\\n",
        "                                           window=np.hamming(nsc), cmap='viridis')\n",
        "        plt.close()\n",
        "        imgsFault[i, :, :] = 10 * np.log10(Pxx[0:imgSize, :])\n",
        "        \n",
        "    np.save(pathFault + '/' + folderFault + '.npy', imgsFault)\n",
        "        \n",
        "print('Fault Image Shape: {}'.format(imgsFault.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fault Image Shape: (1000, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e91QsAurSkIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "a50b5372-5ed6-4bce-c812-eaab7fa68834"
      },
      "cell_type": "code",
      "source": [
        "# Change name from imgsF1 or imgsF5 to imgsNormal and imgsFault\n",
        "\n",
        "dataNumNormal = len(imgsNormal)\n",
        "dataNumFault = len(imgsFault)\n",
        "dataNumNormalTrain = int(dataNumNormal * trainingRatio)\n",
        "dataNumFaultTrain = int(dataNumFault * trainingRatio)\n",
        "dataNumNormalTest = dataNumNormal - dataNumNormalTrain\n",
        "dataNumFaultTest = dataNumFault - dataNumFaultTrain\n",
        "\n",
        "print('Normal Train:Test = {:d}:{:d}'.format(dataNumNormalTrain, dataNumNormalTest))\n",
        "print('Fault  Train:Test = {:d}:{:d}\\n'.format(dataNumFaultTrain, dataNumFaultTest))\n",
        "\n",
        "trainIdxNormal = np.random.choice(dataNumNormal - 1, dataNumNormalTrain, replace=False)\n",
        "testIdxNormal = list(set(range(0, dataNumNormal)) - set(trainIdxNormal))\n",
        "\n",
        "trainImgsNormal = imgsNormal[trainIdxNormal, :, :]\n",
        "testImgsNormal = imgsNormal[testIdxNormal, :, :]\n",
        "\n",
        "print('Normal Training Image Shape {}'.format(trainImgsNormal.shape))\n",
        "print('Normal Test Image Shape {}\\n'.format(testImgsNormal.shape))\n",
        "\n",
        "trainIdxFault  = np.random.choice(dataNumFault - 1, dataNumFaultTrain, replace=False)\n",
        "testIdxFault = list(set(range(0, dataNumFault)) - set(trainIdxFault))\n",
        "\n",
        "trainImgsFault = imgsFault[trainIdxFault, :, :]\n",
        "testImgsFault = imgsFault[testIdxFault, :, :]\n",
        "\n",
        "print('Fault Training Image Shape {}'.format(trainImgsFault.shape))\n",
        "print('Fault Test Image Shape {}\\n'.format(testImgsFault.shape))\n",
        "\n",
        "trainImgs = np.vstack([trainImgsNormal, trainImgsFault])\n",
        "testImgs = np.vstack([testImgsNormal, testImgsFault])\n",
        "\n",
        "print('Training Image Shape {}'.format(trainImgs.shape))\n",
        "print('Test Image Shape {}'.format(testImgs.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Train:Test = 800:200\n",
            "Fault  Train:Test = 800:200\n",
            "\n",
            "Normal Training Image Shape (800, 224, 224)\n",
            "Normal Test Image Shape (200, 224, 224)\n",
            "\n",
            "Fault Training Image Shape (800, 224, 224)\n",
            "Fault Test Image Shape (200, 224, 224)\n",
            "\n",
            "Training Image Shape (1600, 224, 224)\n",
            "Test Image Shape (400, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "feQ_AHAcUgHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ab66b7de-c495-486d-bf10-fa0917daf8d1"
      },
      "cell_type": "code",
      "source": [
        "trainMean = np.mean(trainImgs)\n",
        "trainStd = np.std(trainImgs)\n",
        "\n",
        "print('Mean of Training Image: {}'.format(trainMean))\n",
        "print('Standard Deviation of Training Image: {}'.format(trainStd))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Training Image: -78.5883603831824\n",
            "Standard Deviation of Training Image: 9.265943997524543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wUi9IuXNUpir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Should Change Norm to Normalized\n",
        "\n",
        "trainImgsNorm = (trainImgs - trainMean) / trainStd\n",
        "testImgsNorm = (testImgs - trainMean) / trainStd\n",
        "\n",
        "trainImgsNorm = trainImgsNorm.reshape(list(trainImgsNorm.shape) + [1])\n",
        "testImgsNorm = testImgsNorm.reshape(list(testImgsNorm.shape) + [1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aklxiPz1Ve4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f5c4c015-f873-45ef-c8ad-becd41f6207c"
      },
      "cell_type": "code",
      "source": [
        "X_train = np.stack([trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0], trainImgsNorm[:, :, :, 0]], axis = -1)\n",
        "X_test = np.stack([testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0], testImgsNorm[:, :, :, 0]], axis = -1)\n",
        "\n",
        "print('X_train Shape: {}'.format(X_train.shape))\n",
        "print('X_test  Shape: {}'.format(X_test.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train Shape: (1600, 224, 224, 3)\n",
            "X_test  Shape: (400, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRgIPzT7K_YP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "63da2b36-8c17-47c0-ec1d-b3d4ae675593"
      },
      "cell_type": "code",
      "source": [
        "trainLabelNormal = np.stack((np.ones(dataNumNormalTrain), np.zeros(dataNumNormalTrain)), axis = -1)\n",
        "testLabelNormal = np.stack((np.ones(dataNumNormalTest), np.zeros(dataNumNormalTest)), axis = -1)\n",
        "\n",
        "trainLabelFault = np.stack((np.zeros(dataNumFaultTrain), np.ones(dataNumFaultTrain)), axis = -1)\n",
        "testLabelFault = np.stack((np.zeros(dataNumFaultTest), np.ones(dataNumFaultTest)), axis = -1)\n",
        "\n",
        "Y_train = np.vstack((trainLabelNormal, trainLabelFault))\n",
        "Y_test = np.vstack((testLabelNormal, testLabelFault))\n",
        "\n",
        "print('Y_train Normal:Fault = {:d}:{:d}'.format(len(trainLabelNormal), len(trainLabelFault)))\n",
        "print('Y_test  Normal:Fault = {:d}:{:d}'.format(len(testLabelNormal), len(testLabelFault)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train Normal:Fault = 800:800\n",
            "Y_test  Normal:Fault = 200:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kczwZk7Tbp5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "fb8e25d5-db30-4282-ad39-abb8a7babe21"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# pretrainedModel = 'ResNet50'\n",
        "# lastActivation = 'softmax'\n",
        "# lossFunction = 'binary_crossentropy'\n",
        "# sizeBatch = 2\n",
        "# numEpochs = 2\n",
        "# verb = 1\n",
        "\n",
        "# Refresh all background variables\n",
        "K.clear_session()\n",
        "\n",
        "input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "# Building sequential model with name 'model'\n",
        "model = Sequential()\n",
        "\n",
        "# Model selection\n",
        "\n",
        "if (pretrainedModel == 'VGG16'):\n",
        "    \n",
        "    modelWoTop = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "elif (pretrainedModel == 'VGG19'):\n",
        "    \n",
        "    modelWoTop = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif pretrainedModel == 'ResNet50':\n",
        "    \n",
        "    modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif (pretrainedModel == 'InceptionV3'):\n",
        "    modelWoTop = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "elif (pretrainedModel == 'Xception'):\n",
        "    modelWoTop = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "                      \n",
        "elif (pretrainedModel == 'DenseNet169'):\n",
        "\n",
        "    modelWoTop = DenseNet169(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "elif (pretrainedModel == 'DenseNet201'):\n",
        "    modelWoTop = DenseNet201(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "              \n",
        "else:\n",
        "    print('Invalid Pretrained Model Selection')\n",
        "    \n",
        "              \n",
        "\n",
        "# Model compiling\n",
        "\n",
        "print('Compiling Pretrained {} Model'.format(model.layers[0].name))\n",
        "              \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\주환\\appdata\\local\\conda\\conda\\envs\\paper\\lib\\site-packages\\keras_applications\\resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Compiling Pretrained resnet50 Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eA9GZhLGe5X9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "collapsed": true,
        "outputId": "692ceea0-7672-413a-c022-4a061c95429f"
      },
      "cell_type": "code",
      "source": [
        "print('Training Pretrained {} Model'.format(model.layers[0].name))\n",
        "print('Batch Size: {}\\t Epochs: {}\\t\\n'.format(sizeBatch, numEpochs))\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pretrained resnet50 Model\n",
            "Batch Size: 4\t Epochs: 8\t\n",
            "\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 79s 50ms/step - loss: 0.8601 - acc: 0.8831 - val_loss: 0.1083 - val_acc: 0.9900\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.4559 - acc: 0.9494 - val_loss: 0.0303 - val_acc: 0.9975\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.5186 - acc: 0.9563 - val_loss: 0.0541 - val_acc: 0.9950\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.6118 - acc: 0.9506 - val_loss: 2.5516 - val_acc: 0.8325\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.3684 - acc: 0.9694 - val_loss: 9.5654e-04 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.3019 - acc: 0.9788 - val_loss: 5.4914e-04 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.2872 - acc: 0.9806 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 67s 42ms/step - loss: 0.3128 - acc: 0.9781 - val_loss: 0.0087 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cuyY04cKi3ey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6839
        },
        "outputId": "a84843c9-3d2e-4df0-dc8a-5ad8a1047976"
      },
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    # Refresh all background variables\n",
        "    K.clear_session()\n",
        "\n",
        "    input_tensor = Input(shape=(imgSize, imgSize, 3))\n",
        "\n",
        "    # Building sequential model with name 'model'\n",
        "    model = Sequential()\n",
        "\n",
        "    modelWoTop = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    model.add(modelWoTop)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation=lastActivation))\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=sizeBatch, epochs=numEpochs, verbose=1,\n",
        "              validation_data=(X_test, Y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\주환\\appdata\\local\\conda\\conda\\envs\\paper\\lib\\site-packages\\keras_applications\\resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 1.0744 - acc: 0.8906 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.2703 - acc: 0.9719 - val_loss: 8.4331e-04 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.4025 - acc: 0.9631 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.5101 - acc: 0.9575 - val_loss: 4.9925e-04 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.9355 - acc: 0.9325 - val_loss: 0.0468 - val_acc: 0.9950\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.5379 - acc: 0.9537 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.4997 - acc: 0.9644 - val_loss: 9.8219e-04 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.3626 - acc: 0.9744 - val_loss: 1.6759 - val_acc: 0.8950\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 0.8686 - acc: 0.8888 - val_loss: 4.1189 - val_acc: 0.7325\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.3809 - acc: 0.9463 - val_loss: 0.0764 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.6344 - acc: 0.9444 - val_loss: 0.0142 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.3371 - acc: 0.9738 - val_loss: 4.5632e-04 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 0.2919 - acc: 0.9819 - val_loss: 4.3497e-04 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.3616 - acc: 0.9669 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.3632 - acc: 0.9694 - val_loss: 0.0105 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 0.2722 - acc: 0.9831 - val_loss: 5.3478e-04 - val_acc: 1.0000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 73s 46ms/step - loss: 8.0169 - acc: 0.4994 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 63s 40ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0171 - acc: 0.4981 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 7.9263 - acc: 0.5050 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 3.2365 - acc: 0.7675 - val_loss: 3.2516 - val_acc: 0.7950\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.2563 - acc: 0.9769 - val_loss: 9.1059e-04 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3153 - acc: 0.9781 - val_loss: 2.6194e-04 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.5952 - acc: 0.9581 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3452 - acc: 0.9700 - val_loss: 0.0058 - val_acc: 0.9975\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3964 - acc: 0.9731 - val_loss: 0.0051 - val_acc: 0.9975\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3211 - acc: 0.9800 - val_loss: 2.4832e-04 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3630 - acc: 0.9619 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0208 - acc: 0.4975 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0072 - acc: 0.4994 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 1.3272 - acc: 0.8806 - val_loss: 1.7195 - val_acc: 0.8875\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.2842 - acc: 0.8931 - val_loss: 8.4116 - val_acc: 0.4625\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.1181 - acc: 0.9206 - val_loss: 0.1688 - val_acc: 0.9875\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.3408 - acc: 0.9113 - val_loss: 10.9475 - val_acc: 0.1125\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.6428 - acc: 0.8856 - val_loss: 9.2869e-05 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.4824 - acc: 0.9600 - val_loss: 1.6033 - val_acc: 0.9000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.2034 - acc: 0.9812 - val_loss: 5.0381e-04 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 0.2951 - acc: 0.9800 - val_loss: 1.5300e-04 - val_acc: 1.0000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0079 - acc: 0.4994 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0129 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0168 - acc: 0.4994 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0137 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 0.9663 - acc: 0.9137 - val_loss: 0.0802 - val_acc: 0.9950\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 3.2049 - acc: 0.6931 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.9633 - acc: 0.8337 - val_loss: 0.1775 - val_acc: 0.9825\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3454 - acc: 0.9725 - val_loss: 9.2094e-04 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.2110 - acc: 0.9825 - val_loss: 8.5274e-04 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3178 - acc: 0.9775 - val_loss: 1.6083e-04 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3449 - acc: 0.9775 - val_loss: 0.0054 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 0.2209 - acc: 0.9838 - val_loss: 0.0062 - val_acc: 0.9975\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0062 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 1.0649 - acc: 0.8969 - val_loss: 0.0556 - val_acc: 0.9950\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 0.1701 - acc: 0.9887 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.1792 - acc: 0.9862 - val_loss: 0.0094 - val_acc: 0.9975\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3257 - acc: 0.9725 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.2921 - acc: 0.9812 - val_loss: 1.9952e-04 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 2.3117 - acc: 0.8475 - val_loss: 7.4040 - val_acc: 0.5350\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.5640 - acc: 0.8944 - val_loss: 4.0909e-04 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.2010 - acc: 0.9875 - val_loss: 6.8870e-04 - val_acc: 1.0000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 8.0052 - acc: 0.5006 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 1.1256 - acc: 0.8956 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 0.4989 - acc: 0.9613 - val_loss: 0.0959 - val_acc: 0.9875\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.3398 - acc: 0.9762 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 2.3493 - acc: 0.8200 - val_loss: 2.8234 - val_acc: 0.8150\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.7584 - acc: 0.8881 - val_loss: 0.0754 - val_acc: 0.9825\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 1.2825 - acc: 0.9044 - val_loss: 0.1239 - val_acc: 0.9925\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 61s 38ms/step - loss: 0.2823 - acc: 0.9781 - val_loss: 1.7046e-04 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.5736 - acc: 0.9556 - val_loss: 7.6529e-04 - val_acc: 1.0000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 45ms/step - loss: 8.0083 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 71s 45ms/step - loss: 8.0101 - acc: 0.4988 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 7.9280 - acc: 0.5050 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 38ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1600/1600 [==============================] - 72s 45ms/step - loss: 0.7833 - acc: 0.9131 - val_loss: 0.0068 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.0968 - acc: 0.9894 - val_loss: 3.1858e-04 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 0.0106 - acc: 0.9994 - val_loss: 3.2829e-04 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 3.3146 - acc: 0.7444 - val_loss: 8.2155 - val_acc: 0.4875\n",
            "Epoch 5/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 2.9000 - acc: 0.8006 - val_loss: 3.8624 - val_acc: 0.7025\n",
            "Epoch 6/8\n",
            "1600/1600 [==============================] - 62s 39ms/step - loss: 2.4527 - acc: 0.8387 - val_loss: 8.0151 - val_acc: 0.5000\n",
            "Epoch 7/8\n",
            "1600/1600 [==============================] - 63s 39ms/step - loss: 1.0188 - acc: 0.9225 - val_loss: 0.1228 - val_acc: 0.9925\n",
            "Epoch 8/8\n",
            "1600/1600 [==============================] - 65s 41ms/step - loss: 0.9359 - acc: 0.9331 - val_loss: 1.0408 - val_acc: 0.9350\n",
            "Train on 1600 samples, validate on 400 samples\n",
            "Epoch 1/8\n",
            "1116/1600 [===================>..........] - ETA: 22s - loss: 0.6098 - acc: 0.9435"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qexmaPZVz04q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "fea2e67b-0646-4dec-9cc7-2dd852ec7610"
      },
      "cell_type": "code",
      "source": [
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(Y_test[:, 1], 'r')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(Y_pred[:, 1], 'b')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(Y_test[:, 1] - Y_pred[:, 1], 'g')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IPTFOJh3DTgq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5c4a10c9-2051-46a7-bcac-747cd394628d"
      },
      "cell_type": "code",
      "source": [
        "# : is not allowed in file name in windows OS\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "modelSaved = '{}-{}_{}_{}.h5'.format(folderNormal, folderFault, pretrainedModel, now.strftime('%m-%d-%H-%M-%S'))\n",
        "meanSaved = 'mean_{}.npy'.format(now.strftime('%m-%d-%H-%M-%S'))\n",
        "stdSaved = 'std_{}.npy'.format(now.strftime('%m-%d-%H-%M-%S'))\n",
        "\n",
        "inputStr = input('''Save Model as '{}'? (y/n)\\n'''.format(modelSaved))\n",
        "\n",
        "if (inputStr == 'y' or inputStr == 'Y'):  \n",
        "    \n",
        "    np.save(modelPath + '/' + meanSaved, trainMean)\n",
        "    np.save(modelPath + '/' + stdSaved, trainStd)\n",
        "    model.save(modelPath + '/' + modelSaved)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Model as 'A1F3P3-A5F3P3_Xception_10-08-19-21-36.h5'? (y/n)\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}